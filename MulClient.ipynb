{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-01T03:14:20.197650600Z",
     "start_time": "2023-09-01T03:14:20.171125400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'tensor([[-0.2790, -0.3112,  2.9187,  0.2665,  1.2651, -0.9435,  1.3145,  1.3848,\\n          0.7869]])  1  tensor(False)'"
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "import torch\n",
    "\n",
    "Carbin_Count = 9  # \n",
    "\n",
    "\n",
    "#定义环境\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # 每个船舱的重量\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.state = torch.randn(1, Carbin_Count)  + torch.full((1, Carbin_Count), 1).resize(1,\n",
    "                                                                                                         Carbin_Count)\n",
    "        return self.state\n",
    "\n",
    "    def is_over(self):\n",
    "        # 判断state每一项是否小于0\n",
    "        return torch.all(self.state < 0)\n",
    "\n",
    "    def get_state_reword(self, action: torch.Tensor):\n",
    "        # 当前状态的分数,比如为负数了还有人来卸载就应该扣分\n",
    "        reword = 0\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            if self.state[0][index] > 0:\n",
    "                reword += 1\n",
    "        if self.is_over():\n",
    "            # 惩罚时间 \n",
    "            reword += 1000 - self.time_spend*8\n",
    "        return reword\n",
    "\n",
    "    def step(self, action):\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            self.state[0][index] -= 0.1\n",
    "        self.time_spend += 1\n",
    "        reword = self.get_state_reword(action)\n",
    "        return self.state, reword, self.is_over(), None\n",
    "\n",
    "\n",
    "class MyWrapper:\n",
    "    N = 3  # 港机数量\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(action)\n",
    "        state, reward, terminated, info = self.env.step(action)\n",
    "        over = terminated\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n > 200:\n",
    "            over = True\n",
    "        return state, reward, over\n",
    "\n",
    "    def to_show_state(self):\n",
    "        return '%s  %s  %s' % (str(self.env.state), str(self.env.time_spend), str(self.env.is_over()))\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "env.step(torch.full((1, 1), 0.5, dtype=torch.float32))\n",
    "env.to_show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [
    {
     "data": {
      "text/plain": "[<__main__.A2C at 0x169041a4340>,\n <__main__.A2C at 0x169041a43d0>,\n <__main__.A2C at 0x169041a4520>]"
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class A2C:\n",
    "\n",
    "    def __init__(self, model_actor, model_critic, model_critic_delay,\n",
    "                 optimizer_actor, optimizer_critic):\n",
    "        self.model_actor = model_actor\n",
    "        self.model_critic = model_critic\n",
    "        self.model_critic_delay = model_critic_delay\n",
    "        self.optimizer_actor = optimizer_actor\n",
    "        self.optimizer_critic = optimizer_critic\n",
    "\n",
    "        self.model_critic_delay.load_state_dict(self.model_critic.state_dict())\n",
    "        self.requires_grad(self.model_critic_delay, False)\n",
    "\n",
    "    def soft_update(self, _from, _to):\n",
    "        for _from, _to in zip(_from.parameters(), _to.parameters()):\n",
    "            value = _to.data * 0.99 + _from.data * 0.01\n",
    "            _to.data.copy_(value)\n",
    "\n",
    "    def requires_grad(self, model, value):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(value)\n",
    "\n",
    "    def train_critic(self, state, reward, next_state, over):\n",
    "        self.requires_grad(self.model_critic, True)\n",
    "        self.requires_grad(self.model_actor, False)\n",
    "\n",
    "        #计算values和targets\n",
    "        value = self.model_critic(state)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target = self.model_critic_delay(next_state)\n",
    "        target = target * 0.99 * (1 - over) + reward\n",
    "        # print('xxxx', value.size(), target.size(), reward.size())\n",
    "        #时序差分误差,也就是tdloss\n",
    "        loss = torch.nn.functional.mse_loss(value, target)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        self.soft_update(self.model_critic, self.model_critic_delay)\n",
    "\n",
    "        #减去value相当于去基线\n",
    "        return (target - value).detach()\n",
    "\n",
    "    def train_actor(self, state, action, value):\n",
    "        self.requires_grad(self.model_critic, False)\n",
    "        self.requires_grad(self.model_actor, True)\n",
    "\n",
    "        #重新计算动作的概率\n",
    "        prob = self.model_actor(state)\n",
    "        prob = prob.gather(dim=1, index=action)\n",
    "\n",
    "        #根据策略梯度算法的导函数实现\n",
    "        #函数中的Q(state,action),这里使用critic模型估算\n",
    "        prob = (prob + 1e-8).log() * value\n",
    "        loss = -prob.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "model_actor = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 9),\n",
    "        torch.nn.Softmax(dim=1),\n",
    "    ) for _ in range(env.N)\n",
    "]\n",
    "\n",
    "model_critic, model_critic_delay = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 1),\n",
    "    ) for _ in range(2)\n",
    "]\n",
    "\n",
    "optimizer_actor = [\n",
    "    torch.optim.Adam(model_actor[i].parameters(), lr=1e-3)\n",
    "    for i in range(env.N)\n",
    "]\n",
    "optimizer_critic = torch.optim.Adam(model_critic.parameters(), lr=5e-3)\n",
    "\n",
    "a2c = [\n",
    "    A2C(model_actor[i], model_critic, model_critic_delay, optimizer_actor[i],\n",
    "        optimizer_critic) for i in range(env.N)\n",
    "]\n",
    "# x = torch.FloatTensor([1,2,3,4,6,7,8,8,4]).resize(1,Carbin_Count)\n",
    "# print(model_actor[0](x))\n",
    "model_actor = None\n",
    "model_critic = None\n",
    "model_critic_delay = None\n",
    "optimizer_actor = None\n",
    "optimizer_critic = None\n",
    "\n",
    "a2c\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T03:14:20.245644400Z",
     "start_time": "2023-09-01T03:14:20.201644200Z"
    }
   },
   "id": "955e583b006b55e6"
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [
    {
     "data": {
      "text/plain": "(489.0, torch.Size([72, 1, 9]), torch.Size([72, 1, 1]), torch.Size([72, 3, 1]))"
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        a = []\n",
    "        for i in range(env.N):\n",
    "            #计算动作\n",
    "            prob = a2c[i].model_actor(torch.FloatTensor(s).reshape(\n",
    "                1, -1))[0].tolist()\n",
    "            # print(s, prob)\n",
    "            a.append(random.choices(range(9), weights=prob, k=1)[0])\n",
    "\n",
    "        #执行动作\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            print(env.to_show_state())\n",
    "    # print(state[0])\n",
    "    # print(type(state), len(state))\n",
    "    state = torch.tensor([item.numpy() for item in state])\n",
    "    action = torch.LongTensor(action).unsqueeze(-1)\n",
    "    reward = torch.FloatTensor(reward).unsqueeze(-1).unsqueeze(-1)\n",
    "    next_state = torch.tensor([item.numpy() for item in next_state])\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum,state.size(),reward.size(),action.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T03:14:20.298644700Z",
     "start_time": "2023-09-01T03:14:20.236644300Z"
    }
   },
   "id": "e01ea439facc25cf"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -7.442286014556885 455.85\n",
      "250 19.779632568359375 193.4\n",
      "500 -3.0283079147338867 161.0\n",
      "750 7.787428855895996 173.85\n",
      "1000 -12.408726692199707 66.45\n",
      "1250 1.0780302286148071 67.25\n",
      "1500 1.100326657295227 67.4\n",
      "1750 -0.21749255061149597 106.15\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    #训练N局\n",
    "    for epoch in range(3_000):\n",
    "        state, action, reward, next_state, over, _ = play()\n",
    "\n",
    "        #合并部分字段\n",
    "        state_c = state.flatten(start_dim=1)\n",
    "        reward_c = reward.sum(dim=1)\n",
    "        next_state_c = next_state.flatten(start_dim=1)\n",
    "\n",
    "        for i in range(env.N):\n",
    "            value = a2c[i].train_critic(state_c, reward_c, next_state_c, over)\n",
    "            loss = a2c[i].train_actor(state_c, action[:, i], value)\n",
    "\n",
    "        if epoch % 250 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-09-01T03:14:20.298644700Z"
    }
   },
   "id": "660f338bb74d7ff2"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "play(True)[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9c4e49ce22f9f053"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
