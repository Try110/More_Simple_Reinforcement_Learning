{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-04T05:07:10.655016700Z",
     "start_time": "2023-09-04T05:07:10.604180300Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'tensor([[1.3996, 1.9350, 1.5702, 1.3020, 1.1738, 1.0413, 1.0137, 1.6923, 1.3770]])  步骤:1  游戏是否结束:0'"
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "Carbin_Count = 9  # \n",
    "\n",
    "\n",
    "#定义环境\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # 每个船舱的重量\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.state = torch.FloatTensor(np.random.uniform(1, 2, (1, Carbin_Count)))\n",
    "        return self.state\n",
    "\n",
    "    def is_over(self, action: torch.Tensor = []):\n",
    "        def is_fault(action: torch.Tensor):\n",
    "            last_action = -1\n",
    "            for i in action:\n",
    "                index = int(i)  # 映射到货轮\n",
    "                if index == 0:\n",
    "                    continue\n",
    "                if index < last_action:\n",
    "                    return True\n",
    "                last_action = index\n",
    "            return False\n",
    "\n",
    "        if is_fault(action):\n",
    "            return -1\n",
    "        # 判断state每一项是否小于0\n",
    "        if torch.all(self.state < 0):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def get_state_reword(self, action: torch.Tensor):\n",
    "        # 当前状态的分数,比如为负数了还有人来卸载就应该扣分\n",
    "        reword = 0\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            if index == 0:\n",
    "                reword -= 3\n",
    "                continue\n",
    "            # 动起来的奖励\n",
    "            reword += 1 * 5\n",
    "            if self.state[0][index - 1] < 0:\n",
    "                reword += 5 * (self.state[0][index - 1] * 13)\n",
    "        is_over = self.is_over(action)\n",
    "        if is_over == 1:\n",
    "            reword += 1200\n",
    "        elif is_over == -1:\n",
    "            reword -= 200\n",
    "        return reword\n",
    "\n",
    "    def step(self, action):\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            if index == 0:\n",
    "                continue\n",
    "            self.state[0][index - 1] -= 0.1\n",
    "        self.time_spend += 1\n",
    "        reword = self.get_state_reword(action)\n",
    "        return self.state, reword, self.is_over(action) != 0, None\n",
    "\n",
    "\n",
    "class MyWrapper:\n",
    "    N = 3  # 港机数量\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(action)\n",
    "        state, reward, terminated, info = self.env.step(action)\n",
    "        over = terminated\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n > 200:\n",
    "            over = True\n",
    "        return state, reward, over\n",
    "\n",
    "    def to_show_state(self):\n",
    "        return '%s  步骤:%s  游戏是否结束:%s' % (str(self.env.state), str(self.env.time_spend), str(self.env.is_over()))\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "env.step(torch.full((1, 1), 0.5, dtype=torch.float32))\n",
    "env.to_show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "outputs": [
    {
     "data": {
      "text/plain": "[<__main__.A2C at 0x1e1304abca0>,\n <__main__.A2C at 0x1e1304abd30>,\n <__main__.A2C at 0x1e1304abe80>]"
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class A2C:\n",
    "\n",
    "    def __init__(self, model_actor, model_critic, model_critic_delay,\n",
    "                 optimizer_actor, optimizer_critic):\n",
    "        self.model_actor = model_actor\n",
    "        self.model_critic = model_critic\n",
    "        self.model_critic_delay = model_critic_delay\n",
    "        self.optimizer_actor = optimizer_actor\n",
    "        self.optimizer_critic = optimizer_critic\n",
    "\n",
    "        self.model_critic_delay.load_state_dict(self.model_critic.state_dict())\n",
    "        self.requires_grad(self.model_critic_delay, False)\n",
    "\n",
    "    def soft_update(self, _from, _to):\n",
    "        for _from, _to in zip(_from.parameters(), _to.parameters()):\n",
    "            value = _to.data * 0.99 + _from.data * 0.01\n",
    "            _to.data.copy_(value)\n",
    "\n",
    "    def requires_grad(self, model, value):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(value)\n",
    "\n",
    "    def train_critic(self, state, reward, next_state, over):\n",
    "        self.requires_grad(self.model_critic, True)\n",
    "        self.requires_grad(self.model_actor, False)\n",
    "\n",
    "        #计算values和targets\n",
    "        value = self.model_critic(state)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target = self.model_critic_delay(next_state)\n",
    "        target = target * 0.99 * (1 - over) + reward\n",
    "        # print('xxxx', value.size(), target.size(), reward.size())\n",
    "        #时序差分误差,也就是tdloss\n",
    "        loss = torch.nn.functional.mse_loss(value, target)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        self.soft_update(self.model_critic, self.model_critic_delay)\n",
    "\n",
    "        #减去value相当于去基线\n",
    "        return (target - value).detach()\n",
    "\n",
    "    def train_actor(self, state, action, value):\n",
    "        self.requires_grad(self.model_critic, False)\n",
    "        self.requires_grad(self.model_actor, True)\n",
    "\n",
    "        #重新计算动作的概率\n",
    "        prob = self.model_actor(state)\n",
    "        prob = prob.gather(dim=1, index=action)\n",
    "\n",
    "        #根据策略梯度算法的导函数实现\n",
    "        #函数中的Q(state,action),这里使用critic模型估算\n",
    "        prob = (prob + 1e-8).log() * value\n",
    "        loss = -prob.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "model_actor = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, Carbin_Count + 1),\n",
    "        torch.nn.Softmax(dim=1),\n",
    "    ) for _ in range(env.N)\n",
    "]\n",
    "\n",
    "model_critic, model_critic_delay = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 1),\n",
    "    ) for _ in range(2)\n",
    "]\n",
    "\n",
    "optimizer_actor = [\n",
    "    torch.optim.Adam(model_actor[i].parameters(), lr=1e-3)\n",
    "    for i in range(env.N)\n",
    "]\n",
    "optimizer_critic = torch.optim.Adam(model_critic.parameters(), lr=5e-3)\n",
    "\n",
    "a2c = [\n",
    "    A2C(model_actor[i], model_critic, model_critic_delay, optimizer_actor[i],\n",
    "        optimizer_critic) for i in range(env.N)\n",
    "]\n",
    "# x = torch.FloatTensor([1,2,3,4,6,7,8,8,4]).resize(1,Carbin_Count)\n",
    "# print(model_actor[0](x))\n",
    "model_actor = None\n",
    "model_critic = None\n",
    "model_critic_delay = None\n",
    "optimizer_actor = None\n",
    "optimizer_critic = None\n",
    "\n",
    "a2c\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T05:07:10.655016700Z",
     "start_time": "2023-09-04T05:07:10.621216600Z"
    }
   },
   "id": "955e583b006b55e6"
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "outputs": [
    {
     "data": {
      "text/plain": "(-193.0, torch.Size([1, 1, 9]), torch.Size([1, 1, 1]), torch.Size([1, 3, 1]))"
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        a = []\n",
    "        for i in range(env.N):\n",
    "            #计算动作\n",
    "            prob = a2c[i].model_actor(torch.FloatTensor(s).reshape(\n",
    "                1, -1))[0].tolist()\n",
    "            # print(s, prob)\n",
    "            a.append(random.choices(range(Carbin_Count + 1), weights=prob, k=1)[0])\n",
    "\n",
    "        #执行动作\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            print(a, env.to_show_state())\n",
    "    # print(state[0])\n",
    "    # print(type(state), len(state))\n",
    "    state = torch.tensor([item.numpy() for item in state])\n",
    "    action = torch.LongTensor(action).unsqueeze(-1)\n",
    "    reward = torch.FloatTensor(reward).unsqueeze(-1).unsqueeze(-1)\n",
    "    next_state = torch.tensor([item.numpy() for item in next_state])\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum, state.size(), reward.size(), action.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T05:07:10.711332Z",
     "start_time": "2023-09-04T05:07:10.661031700Z"
    }
   },
   "id": "e01ea439facc25cf"
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -477.1976623535156 -181.75\n",
      "250 -5.270344257354736 -151.05\n",
      "500 -1.5087164640426636 -572.780606842041\n",
      "750 0.028594359755516052 -215.3936586380005\n",
      "1000 -21.804866790771484 -210.26653938293458\n",
      "1250 40.616859436035156 -1357.6139625549317\n",
      "1500 1.8938491344451904 866.8003646850586\n",
      "1750 -3322.0166015625 -4423.916413116455\n",
      "2000 24.359785079956055 1177.1616806030274\n",
      "2250 -24.82537078857422 1206.2004661560059\n",
      "2500 3.3432013988494873 1600.9697021484376\n",
      "2750 -5.417761325836182 1616.6085876464845\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    #训练N局\n",
    "    for epoch in range(3_000):\n",
    "        state, action, reward, next_state, over, _ = play()\n",
    "\n",
    "        #合并部分字段\n",
    "        state_c = state.flatten(start_dim=1)\n",
    "        reward_c = reward.sum(dim=1)\n",
    "        next_state_c = next_state.flatten(start_dim=1)\n",
    "\n",
    "        for i in range(env.N):\n",
    "            value = a2c[i].train_critic(state_c, reward_c, next_state_c, over)\n",
    "            loss = a2c[i].train_actor(state_c, action[:, i], value)\n",
    "\n",
    "        if epoch % 250 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T05:08:43.814473600Z",
     "start_time": "2023-09-04T05:07:10.684172400Z"
    }
   },
   "id": "660f338bb74d7ff2"
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 4, 9] tensor([[1.5364, 1.3467, 1.0137, 1.5024, 1.8272, 1.6970, 1.9312, 1.1900, 1.6986]])  步骤:1  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[1.4364, 1.3467, 1.0137, 1.5024, 1.7272, 1.6970, 1.9312, 1.1900, 1.5986]])  步骤:2  游戏是否结束:0\n",
      "[1, 6, 9] tensor([[1.3364, 1.3467, 1.0137, 1.5024, 1.7272, 1.5970, 1.9312, 1.1900, 1.4986]])  步骤:3  游戏是否结束:0\n",
      "[1, 6, 9] tensor([[1.2364, 1.3467, 1.0137, 1.5024, 1.7272, 1.4970, 1.9312, 1.1900, 1.3986]])  步骤:4  游戏是否结束:0\n",
      "[1, 4, 9] tensor([[1.1364, 1.3467, 1.0137, 1.4024, 1.7272, 1.4970, 1.9312, 1.1900, 1.2986]])  步骤:5  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[1.0364, 1.3467, 1.0137, 1.4024, 1.6272, 1.4970, 1.9312, 1.1900, 1.1986]])  步骤:6  游戏是否结束:0\n",
      "[1, 4, 9] tensor([[0.9364, 1.3467, 1.0137, 1.3024, 1.6272, 1.4970, 1.9312, 1.1900, 1.0986]])  步骤:7  游戏是否结束:0\n",
      "[1, 6, 9] tensor([[0.8364, 1.3467, 1.0137, 1.3024, 1.6272, 1.3970, 1.9312, 1.1900, 0.9986]])  步骤:8  游戏是否结束:0\n",
      "[1, 4, 9] tensor([[0.7364, 1.3467, 1.0137, 1.2024, 1.6272, 1.3970, 1.9312, 1.1900, 0.8986]])  步骤:9  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.6364, 1.3467, 1.0137, 1.2024, 1.5272, 1.3970, 1.9312, 1.1900, 0.7986]])  步骤:10  游戏是否结束:0\n",
      "[1, 4, 9] tensor([[0.5364, 1.3467, 1.0137, 1.1024, 1.5272, 1.3970, 1.9312, 1.1900, 0.6986]])  步骤:11  游戏是否结束:0\n",
      "[1, 6, 9] tensor([[0.4364, 1.3467, 1.0137, 1.1024, 1.5272, 1.2970, 1.9312, 1.1900, 0.5986]])  步骤:12  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.3364, 1.3467, 1.0137, 1.1024, 1.4272, 1.2970, 1.9312, 1.1900, 0.4986]])  步骤:13  游戏是否结束:0\n",
      "[1, 6, 9] tensor([[0.2364, 1.3467, 1.0137, 1.1024, 1.4272, 1.1970, 1.9312, 1.1900, 0.3986]])  步骤:14  游戏是否结束:0\n",
      "[1, 5, 8] tensor([[0.1364, 1.3467, 1.0137, 1.1024, 1.3272, 1.1970, 1.9312, 1.0900, 0.3986]])  步骤:15  游戏是否结束:0\n",
      "[1, 5, 7] tensor([[0.0364, 1.3467, 1.0137, 1.1024, 1.2272, 1.1970, 1.8312, 1.0900, 0.3986]])  步骤:16  游戏是否结束:0\n",
      "[2, 4, 9] tensor([[0.0364, 1.2467, 1.0137, 1.0024, 1.2272, 1.1970, 1.8312, 1.0900, 0.2986]])  步骤:17  游戏是否结束:0\n",
      "[2, 6, 9] tensor([[0.0364, 1.1467, 1.0137, 1.0024, 1.2272, 1.0970, 1.8312, 1.0900, 0.1986]])  步骤:18  游戏是否结束:0\n",
      "[3, 5, 8] tensor([[0.0364, 1.1467, 0.9137, 1.0024, 1.1272, 1.0970, 1.8312, 0.9900, 0.1986]])  步骤:19  游戏是否结束:0\n",
      "[3, 5, 7] tensor([[0.0364, 1.1467, 0.8137, 1.0024, 1.0272, 1.0970, 1.7312, 0.9900, 0.1986]])  步骤:20  游戏是否结束:0\n",
      "[2, 6, 7] tensor([[0.0364, 1.0467, 0.8137, 1.0024, 1.0272, 0.9970, 1.6312, 0.9900, 0.1986]])  步骤:21  游戏是否结束:0\n",
      "[3, 4, 7] tensor([[0.0364, 1.0467, 0.7137, 0.9024, 1.0272, 0.9970, 1.5312, 0.9900, 0.1986]])  步骤:22  游戏是否结束:0\n",
      "[2, 6, 8] tensor([[0.0364, 0.9467, 0.7137, 0.9024, 1.0272, 0.8970, 1.5312, 0.8900, 0.1986]])  步骤:23  游戏是否结束:0\n",
      "[2, 6, 7] tensor([[0.0364, 0.8467, 0.7137, 0.9024, 1.0272, 0.7970, 1.4312, 0.8900, 0.1986]])  步骤:24  游戏是否结束:0\n",
      "[2, 4, 7] tensor([[0.0364, 0.7467, 0.7137, 0.8024, 1.0272, 0.7970, 1.3312, 0.8900, 0.1986]])  步骤:25  游戏是否结束:0\n",
      "[2, 5, 8] tensor([[0.0364, 0.6467, 0.7137, 0.8024, 0.9272, 0.7970, 1.3312, 0.7900, 0.1986]])  步骤:26  游戏是否结束:0\n",
      "[3, 4, 7] tensor([[0.0364, 0.6467, 0.6137, 0.7024, 0.9272, 0.7970, 1.2312, 0.7900, 0.1986]])  步骤:27  游戏是否结束:0\n",
      "[2, 5, 8] tensor([[0.0364, 0.5467, 0.6137, 0.7024, 0.8272, 0.7970, 1.2312, 0.6900, 0.1986]])  步骤:28  游戏是否结束:0\n",
      "[2, 5, 0] tensor([[0.0364, 0.4467, 0.6137, 0.7024, 0.7272, 0.7970, 1.2312, 0.6900, 0.1986]])  步骤:29  游戏是否结束:0\n",
      "[3, 5, 8] tensor([[0.0364, 0.4467, 0.5137, 0.7024, 0.6272, 0.7970, 1.2312, 0.5900, 0.1986]])  步骤:30  游戏是否结束:0\n",
      "[1, 6, 9] tensor([[-0.0636,  0.4467,  0.5137,  0.7024,  0.6272,  0.6970,  1.2312,  0.5900,\n",
      "          0.0986]])  步骤:31  游戏是否结束:0\n",
      "[2, 6, 7] tensor([[-0.0636,  0.3467,  0.5137,  0.7024,  0.6272,  0.5970,  1.1312,  0.5900,\n",
      "          0.0986]])  步骤:32  游戏是否结束:0\n",
      "[3, 4, 7] tensor([[-0.0636,  0.3467,  0.4137,  0.6024,  0.6272,  0.5970,  1.0312,  0.5900,\n",
      "          0.0986]])  步骤:33  游戏是否结束:0\n",
      "[2, 6, 8] tensor([[-0.0636,  0.2467,  0.4137,  0.6024,  0.6272,  0.4970,  1.0312,  0.4900,\n",
      "          0.0986]])  步骤:34  游戏是否结束:0\n",
      "[2, 5, 7] tensor([[-0.0636,  0.1467,  0.4137,  0.6024,  0.5272,  0.4970,  0.9312,  0.4900,\n",
      "          0.0986]])  步骤:35  游戏是否结束:0\n",
      "[2, 5, 7] tensor([[-0.0636,  0.0467,  0.4137,  0.6024,  0.4272,  0.4970,  0.8312,  0.4900,\n",
      "          0.0986]])  步骤:36  游戏是否结束:0\n",
      "[2, 4, 7] tensor([[-0.0636, -0.0533,  0.4137,  0.5024,  0.4272,  0.4970,  0.7312,  0.4900,\n",
      "          0.0986]])  步骤:37  游戏是否结束:0\n",
      "[3, 6, 8] tensor([[-0.0636, -0.0533,  0.3137,  0.5024,  0.4272,  0.3970,  0.7312,  0.3900,\n",
      "          0.0986]])  步骤:38  游戏是否结束:0\n",
      "[2, 6, 7] tensor([[-0.0636, -0.1533,  0.3137,  0.5024,  0.4272,  0.2970,  0.6312,  0.3900,\n",
      "          0.0986]])  步骤:39  游戏是否结束:0\n",
      "[3, 5, 7] tensor([[-0.0636, -0.1533,  0.2137,  0.5024,  0.3272,  0.2970,  0.5312,  0.3900,\n",
      "          0.0986]])  步骤:40  游戏是否结束:0\n",
      "[1, 4, 0] tensor([[-0.1636, -0.1533,  0.2137,  0.4024,  0.3272,  0.2970,  0.5312,  0.3900,\n",
      "          0.0986]])  步骤:41  游戏是否结束:0\n",
      "[3, 5, 7] tensor([[-0.1636, -0.1533,  0.1137,  0.4024,  0.2272,  0.2970,  0.4312,  0.3900,\n",
      "          0.0986]])  步骤:42  游戏是否结束:0\n",
      "[3, 6, 7] tensor([[-0.1636, -0.1533,  0.0137,  0.4024,  0.2272,  0.1970,  0.3312,  0.3900,\n",
      "          0.0986]])  步骤:43  游戏是否结束:0\n",
      "[3, 6, 7] tensor([[-0.1636, -0.1533, -0.0863,  0.4024,  0.2272,  0.0970,  0.2312,  0.3900,\n",
      "          0.0986]])  步骤:44  游戏是否结束:0\n",
      "[0, 4, 8] tensor([[-0.1636, -0.1533, -0.0863,  0.3024,  0.2272,  0.0970,  0.2312,  0.2900,\n",
      "          0.0986]])  步骤:45  游戏是否结束:0\n",
      "[3, 4, 8] tensor([[-0.1636, -0.1533, -0.1863,  0.2024,  0.2272,  0.0970,  0.2312,  0.1900,\n",
      "          0.0986]])  步骤:46  游戏是否结束:0\n",
      "[3, 4, 8] tensor([[-0.1636, -0.1533, -0.2863,  0.1024,  0.2272,  0.0970,  0.2312,  0.0900,\n",
      "          0.0986]])  步骤:47  游戏是否结束:0\n",
      "[2, 5, 8] tensor([[-0.1636, -0.2533, -0.2863,  0.1024,  0.1272,  0.0970,  0.2312, -0.0100,\n",
      "          0.0986]])  步骤:48  游戏是否结束:0\n",
      "[1, 6, 8] tensor([[-0.2636, -0.2533, -0.2863,  0.1024,  0.1272, -0.0030,  0.2312, -0.1100,\n",
      "          0.0986]])  步骤:49  游戏是否结束:0\n",
      "[3, 4, 7] tensor([[-0.2636, -0.2533, -0.3863,  0.0024,  0.1272, -0.0030,  0.1312, -0.1100,\n",
      "          0.0986]])  步骤:50  游戏是否结束:0\n",
      "[3, 7, 7] tensor([[-0.2636, -0.2533, -0.4863,  0.0024,  0.1272, -0.0030, -0.0688, -0.1100,\n",
      "          0.0986]])  步骤:51  游戏是否结束:0\n",
      "[2, 4, 8] tensor([[-0.2636, -0.3533, -0.4863, -0.0976,  0.1272, -0.0030, -0.0688, -0.2100,\n",
      "          0.0986]])  步骤:52  游戏是否结束:0\n",
      "[3, 4, 9] tensor([[-0.2636, -0.3533, -0.5863, -0.1976,  0.1272, -0.0030, -0.0688, -0.2100,\n",
      "         -0.0014]])  步骤:53  游戏是否结束:0\n",
      "[3, 5, 9] tensor([[-0.2636, -0.3533, -0.6863, -0.1976,  0.0272, -0.0030, -0.0688, -0.2100,\n",
      "         -0.1014]])  步骤:54  游戏是否结束:0\n",
      "[2, 4, 7] tensor([[-0.2636, -0.4533, -0.6863, -0.2976,  0.0272, -0.0030, -0.1688, -0.2100,\n",
      "         -0.1014]])  步骤:55  游戏是否结束:0\n",
      "[2, 5, 8] tensor([[-0.2636, -0.5533, -0.6863, -0.2976, -0.0728, -0.0030, -0.1688, -0.3100,\n",
      "         -0.1014]])  步骤:56  游戏是否结束:1\n"
     ]
    },
    {
     "data": {
      "text/plain": "1578.3857421875"
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T05:48:27.313345300Z",
     "start_time": "2023-09-04T05:48:27.244316100Z"
    }
   },
   "id": "9c4e49ce22f9f053"
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-04T05:08:43.943144500Z",
     "start_time": "2023-09-04T05:08:43.883284400Z"
    }
   },
   "id": "7a3acb85b94d3e07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
