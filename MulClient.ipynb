{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-01T09:07:56.889954Z",
     "start_time": "2023-09-01T09:07:56.854954100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'tensor([[1.9172, 1.7847, 1.2609, 1.9336, 1.4298, 1.9052, 1.9956, 1.1288, 1.0310]])  1  tensor(False)'"
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "Carbin_Count = 9  # \n",
    "\n",
    "\n",
    "#定义环境\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # 每个船舱的重量\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.state = torch.FloatTensor(np.random.uniform(1, 2, (1, Carbin_Count)))\n",
    "        return self.state\n",
    "\n",
    "    def is_over(self):\n",
    "        # 判断state每一项是否小于0\n",
    "        return torch.all(self.state < 0)\n",
    "\n",
    "    def get_state_reword(self, action: torch.Tensor):\n",
    "        # 当前状态的分数,比如为负数了还有人来卸载就应该扣分\n",
    "        reword = 0\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            if index == 0:\n",
    "                continue\n",
    "            # 动起来的奖励\n",
    "            reword += 1\n",
    "            if self.state[0][index - 1] < 0:\n",
    "                reword += 1*(self.state[0][index - 1]*13)\n",
    "        if self.is_over():\n",
    "            # 惩罚时间 \n",
    "            reword += 1000\n",
    "        return reword\n",
    "\n",
    "    def step(self, action):\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            if index == 0:\n",
    "                continue\n",
    "            self.state[0][index - 1] -= 0.1\n",
    "        self.time_spend += 1\n",
    "        reword = self.get_state_reword(action)\n",
    "        return self.state, reword, self.is_over(), None\n",
    "\n",
    "\n",
    "class MyWrapper:\n",
    "    N = 3  # 港机数量\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(action)\n",
    "        state, reward, terminated, info = self.env.step(action)\n",
    "        over = terminated\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n > 200:\n",
    "            over = True\n",
    "        return state, reward, over\n",
    "\n",
    "    def to_show_state(self):\n",
    "        return '%s  %s  %s' % (str(self.env.state), str(self.env.time_spend), str(self.env.is_over()))\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "env.step(torch.full((1, 1), 0.5, dtype=torch.float32))\n",
    "env.to_show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "outputs": [
    {
     "data": {
      "text/plain": "[<__main__.A2C at 0x1690789a8e0>,\n <__main__.A2C at 0x1690789a1f0>,\n <__main__.A2C at 0x1690789a9d0>]"
     },
     "execution_count": 215,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class A2C:\n",
    "\n",
    "    def __init__(self, model_actor, model_critic, model_critic_delay,\n",
    "                 optimizer_actor, optimizer_critic):\n",
    "        self.model_actor = model_actor\n",
    "        self.model_critic = model_critic\n",
    "        self.model_critic_delay = model_critic_delay\n",
    "        self.optimizer_actor = optimizer_actor\n",
    "        self.optimizer_critic = optimizer_critic\n",
    "\n",
    "        self.model_critic_delay.load_state_dict(self.model_critic.state_dict())\n",
    "        self.requires_grad(self.model_critic_delay, False)\n",
    "\n",
    "    def soft_update(self, _from, _to):\n",
    "        for _from, _to in zip(_from.parameters(), _to.parameters()):\n",
    "            value = _to.data * 0.99 + _from.data * 0.01\n",
    "            _to.data.copy_(value)\n",
    "\n",
    "    def requires_grad(self, model, value):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(value)\n",
    "\n",
    "    def train_critic(self, state, reward, next_state, over):\n",
    "        self.requires_grad(self.model_critic, True)\n",
    "        self.requires_grad(self.model_actor, False)\n",
    "\n",
    "        #计算values和targets\n",
    "        value = self.model_critic(state)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target = self.model_critic_delay(next_state)\n",
    "        target = target * 0.99 * (1 - over) + reward\n",
    "        # print('xxxx', value.size(), target.size(), reward.size())\n",
    "        #时序差分误差,也就是tdloss\n",
    "        loss = torch.nn.functional.mse_loss(value, target)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        self.soft_update(self.model_critic, self.model_critic_delay)\n",
    "\n",
    "        #减去value相当于去基线\n",
    "        return (target - value).detach()\n",
    "\n",
    "    def train_actor(self, state, action, value):\n",
    "        self.requires_grad(self.model_critic, False)\n",
    "        self.requires_grad(self.model_actor, True)\n",
    "\n",
    "        #重新计算动作的概率\n",
    "        prob = self.model_actor(state)\n",
    "        prob = prob.gather(dim=1, index=action)\n",
    "\n",
    "        #根据策略梯度算法的导函数实现\n",
    "        #函数中的Q(state,action),这里使用critic模型估算\n",
    "        prob = (prob + 1e-8).log() * value\n",
    "        loss = -prob.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "model_actor = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, Carbin_Count + 1),\n",
    "        torch.nn.Softmax(dim=1),\n",
    "    ) for _ in range(env.N)\n",
    "]\n",
    "\n",
    "model_critic, model_critic_delay = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 1),\n",
    "    ) for _ in range(2)\n",
    "]\n",
    "\n",
    "optimizer_actor = [\n",
    "    torch.optim.Adam(model_actor[i].parameters(), lr=1e-3)\n",
    "    for i in range(env.N)\n",
    "]\n",
    "optimizer_critic = torch.optim.Adam(model_critic.parameters(), lr=5e-3)\n",
    "\n",
    "a2c = [\n",
    "    A2C(model_actor[i], model_critic, model_critic_delay, optimizer_actor[i],\n",
    "        optimizer_critic) for i in range(env.N)\n",
    "]\n",
    "# x = torch.FloatTensor([1,2,3,4,6,7,8,8,4]).resize(1,Carbin_Count)\n",
    "# print(model_actor[0](x))\n",
    "model_actor = None\n",
    "model_critic = None\n",
    "model_critic_delay = None\n",
    "optimizer_actor = None\n",
    "optimizer_critic = None\n",
    "\n",
    "a2c\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T09:07:56.905954100Z",
     "start_time": "2023-09-01T09:07:56.889954Z"
    }
   },
   "id": "955e583b006b55e6"
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "outputs": [
    {
     "data": {
      "text/plain": "(690.7562866210938,\n torch.Size([77, 1, 9]),\n torch.Size([77, 1, 1]),\n torch.Size([77, 3, 1]))"
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        a = []\n",
    "        for i in range(env.N):\n",
    "            #计算动作\n",
    "            prob = a2c[i].model_actor(torch.FloatTensor(s).reshape(\n",
    "                1, -1))[0].tolist()\n",
    "            # print(s, prob)\n",
    "            a.append(random.choices(range(Carbin_Count + 1), weights=prob, k=1)[0])\n",
    "\n",
    "        #执行动作\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            print(env.to_show_state())\n",
    "    # print(state[0])\n",
    "    # print(type(state), len(state))\n",
    "    state = torch.tensor([item.numpy() for item in state])\n",
    "    action = torch.LongTensor(action).unsqueeze(-1)\n",
    "    reward = torch.FloatTensor(reward).unsqueeze(-1).unsqueeze(-1)\n",
    "    next_state = torch.tensor([item.numpy() for item in next_state])\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum, state.size(), reward.size(), action.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T09:07:56.981002200Z",
     "start_time": "2023-09-01T09:07:56.905954100Z"
    }
   },
   "id": "e01ea439facc25cf"
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 22.58664321899414 496.131237411499\n",
      "250 -0.0991472527384758 818.4802947998047\n",
      "500 -1.3202204704284668 527.8281219482421\n",
      "750 -1.4276803731918335 634.3708930969239\n",
      "1000 -5.746939182281494 556.3465599060058\n",
      "1250 5.100186824798584 677.1164047241211\n",
      "1500 -3.825230836868286 667.8517028808594\n",
      "1750 -11.097969055175781 627.5514198303223\n",
      "2000 -17.649765014648438 648.3254898071289\n",
      "2250 5.3272013664245605 656.0742874145508\n",
      "2500 5.119644641876221 816.2438751220703\n",
      "2750 -2.265223264694214 793.3619720458985\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    #训练N局\n",
    "    for epoch in range(3_000):\n",
    "        state, action, reward, next_state, over, _ = play()\n",
    "\n",
    "        #合并部分字段\n",
    "        state_c = state.flatten(start_dim=1)\n",
    "        reward_c = reward.sum(dim=1)\n",
    "        next_state_c = next_state.flatten(start_dim=1)\n",
    "\n",
    "        for i in range(env.N):\n",
    "            value = a2c[i].train_critic(state_c, reward_c, next_state_c, over)\n",
    "            loss = a2c[i].train_actor(state_c, action[:, i], value)\n",
    "\n",
    "        if epoch % 250 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T09:11:16.035100800Z",
     "start_time": "2023-09-01T09:07:56.986002200Z"
    }
   },
   "id": "660f338bb74d7ff2"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1.5261, 1.2775, 1.4488, 1.5083, 1.4671, 1.6141, 1.2891, 1.7750, 1.0300]])  1  tensor(False)\n",
      "tensor([[1.4261, 1.2775, 1.4488, 1.5083, 1.3671, 1.6141, 1.2891, 1.6750, 1.0300]])  2  tensor(False)\n",
      "tensor([[1.3261, 1.2775, 1.4488, 1.5083, 1.2671, 1.6141, 1.2891, 1.5750, 1.0300]])  3  tensor(False)\n",
      "tensor([[1.3261, 1.2775, 1.4488, 1.5083, 1.2671, 1.5141, 1.2891, 1.4750, 0.9300]])  4  tensor(False)\n",
      "tensor([[1.3261, 1.2775, 1.3488, 1.5083, 1.2671, 1.5141, 1.2891, 1.3750, 0.8300]])  5  tensor(False)\n",
      "tensor([[1.2261, 1.2775, 1.3488, 1.5083, 1.2671, 1.5141, 1.2891, 1.2750, 0.7300]])  6  tensor(False)\n",
      "tensor([[1.2261, 1.2775, 1.3488, 1.5083, 1.1671, 1.4141, 1.2891, 1.1750, 0.7300]])  7  tensor(False)\n",
      "tensor([[1.1261, 1.2775, 1.3488, 1.4083, 1.0671, 1.4141, 1.2891, 1.1750, 0.7300]])  8  tensor(False)\n",
      "tensor([[1.1261, 1.2775, 1.3488, 1.4083, 0.9671, 1.3141, 1.2891, 1.0750, 0.7300]])  9  tensor(False)\n",
      "tensor([[1.1261, 1.2775, 1.3488, 1.4083, 0.9671, 1.2141, 1.2891, 0.9750, 0.6300]])  10  tensor(False)\n",
      "tensor([[1.1261, 1.2775, 1.2488, 1.4083, 0.8671, 1.2141, 1.2891, 0.8750, 0.6300]])  11  tensor(False)\n",
      "tensor([[1.0261, 1.2775, 1.2488, 1.4083, 0.7671, 1.2141, 1.2891, 0.7750, 0.6300]])  12  tensor(False)\n",
      "tensor([[1.0261, 1.1775, 1.2488, 1.3083, 0.6671, 1.2141, 1.2891, 0.7750, 0.6300]])  13  tensor(False)\n",
      "tensor([[1.0261, 1.1775, 1.2488, 1.2083, 0.6671, 1.1141, 1.2891, 0.7750, 0.5300]])  14  tensor(False)\n",
      "tensor([[1.0261, 1.1775, 1.1488, 1.1083, 0.5671, 1.1141, 1.2891, 0.7750, 0.5300]])  15  tensor(False)\n",
      "tensor([[1.0261, 1.1775, 1.1488, 1.0083, 0.4671, 1.0141, 1.2891, 0.7750, 0.5300]])  16  tensor(False)\n",
      "tensor([[1.0261, 1.0775, 1.1488, 0.9083, 0.3671, 1.0141, 1.2891, 0.7750, 0.5300]])  17  tensor(False)\n",
      "tensor([[1.0261, 1.0775, 1.0488, 0.8083, 0.3671, 1.0141, 1.2891, 0.7750, 0.4300]])  18  tensor(False)\n",
      "tensor([[1.0261, 0.9775, 1.0488, 0.8083, 0.3671, 1.0141, 1.2891, 0.6750, 0.3300]])  19  tensor(False)\n",
      "tensor([[1.0261, 0.9775, 0.9488, 0.7083, 0.3671, 1.0141, 1.2891, 0.6750, 0.2300]])  20  tensor(False)\n",
      "tensor([[1.0261, 0.9775, 0.8488, 0.7083, 0.3671, 1.0141, 1.1891, 0.6750, 0.1300]])  21  tensor(False)\n",
      "tensor([[0.9261, 0.9775, 0.8488, 0.7083, 0.3671, 1.0141, 1.1891, 0.5750, 0.0300]])  22  tensor(False)\n",
      "tensor([[ 0.9261,  0.9775,  0.8488,  0.6083,  0.3671,  0.9141,  1.1891,  0.5750,\n",
      "         -0.0700]])  23  tensor(False)\n",
      "tensor([[ 0.9261,  0.8775,  0.8488,  0.5083,  0.2671,  0.9141,  1.1891,  0.5750,\n",
      "         -0.0700]])  24  tensor(False)\n",
      "tensor([[ 0.9261,  0.8775,  0.8488,  0.5083,  0.2671,  0.8141,  1.0891,  0.5750,\n",
      "         -0.1700]])  25  tensor(False)\n",
      "tensor([[ 0.9261,  0.7775,  0.8488,  0.5083,  0.1671,  0.8141,  0.9891,  0.5750,\n",
      "         -0.1700]])  26  tensor(False)\n",
      "tensor([[ 0.9261,  0.6775,  0.8488,  0.4083,  0.1671,  0.8141,  0.9891,  0.5750,\n",
      "         -0.2700]])  27  tensor(False)\n",
      "tensor([[ 0.8261,  0.6775,  0.8488,  0.4083,  0.0671,  0.8141,  0.8891,  0.5750,\n",
      "         -0.2700]])  28  tensor(False)\n",
      "tensor([[ 0.8261,  0.6775,  0.7488,  0.4083,  0.0671,  0.8141,  0.8891,  0.4750,\n",
      "         -0.3700]])  29  tensor(False)\n",
      "tensor([[ 0.8261,  0.6775,  0.7488,  0.4083,  0.0671,  0.7141,  0.8891,  0.3750,\n",
      "         -0.4700]])  30  tensor(False)\n",
      "tensor([[ 0.7261,  0.6775,  0.7488,  0.3083, -0.0329,  0.7141,  0.8891,  0.3750,\n",
      "         -0.4700]])  31  tensor(False)\n",
      "tensor([[ 0.7261,  0.5775,  0.7488,  0.3083, -0.0329,  0.7141,  0.8891,  0.2750,\n",
      "         -0.5700]])  32  tensor(False)\n",
      "tensor([[ 0.7261,  0.5775,  0.6488,  0.2083, -0.1329,  0.7141,  0.8891,  0.2750,\n",
      "         -0.5700]])  33  tensor(False)\n",
      "tensor([[ 0.7261,  0.5775,  0.6488,  0.1083, -0.2329,  0.6141,  0.8891,  0.2750,\n",
      "         -0.5700]])  34  tensor(False)\n",
      "tensor([[ 0.7261,  0.5775,  0.5488,  0.1083, -0.2329,  0.6141,  0.7891,  0.2750,\n",
      "         -0.6700]])  35  tensor(False)\n",
      "tensor([[ 0.7261,  0.4775,  0.5488,  0.0083, -0.3329,  0.6141,  0.7891,  0.2750,\n",
      "         -0.6700]])  36  tensor(False)\n",
      "tensor([[ 0.7261,  0.4775,  0.4488, -0.0917, -0.3329,  0.6141,  0.7891,  0.2750,\n",
      "         -0.7700]])  37  tensor(False)\n",
      "tensor([[ 0.7261,  0.3775,  0.4488, -0.0917, -0.3329,  0.6141,  0.6891,  0.2750,\n",
      "         -0.8700]])  38  tensor(False)\n",
      "tensor([[ 0.6261,  0.3775,  0.4488, -0.0917, -0.3329,  0.6141,  0.5891,  0.2750,\n",
      "         -0.9700]])  39  tensor(False)\n",
      "tensor([[ 0.5261,  0.3775,  0.4488, -0.0917, -0.3329,  0.6141,  0.4891,  0.2750,\n",
      "         -1.0700]])  40  tensor(False)\n",
      "tensor([[ 0.4261,  0.3775,  0.4488, -0.0917, -0.4329,  0.6141,  0.4891,  0.1750,\n",
      "         -1.0700]])  41  tensor(False)\n",
      "tensor([[ 0.4261,  0.3775,  0.4488, -0.1917, -0.5329,  0.5141,  0.4891,  0.1750,\n",
      "         -1.0700]])  42  tensor(False)\n",
      "tensor([[ 0.3261,  0.3775,  0.4488, -0.2917, -0.6329,  0.5141,  0.4891,  0.1750,\n",
      "         -1.0700]])  43  tensor(False)\n",
      "tensor([[ 0.3261,  0.3775,  0.4488, -0.2917, -0.7329,  0.4141,  0.3891,  0.1750,\n",
      "         -1.0700]])  44  tensor(False)\n",
      "tensor([[ 0.3261,  0.3775,  0.4488, -0.2917, -0.7329,  0.3141,  0.3891,  0.0750,\n",
      "         -1.1700]])  45  tensor(False)\n",
      "tensor([[ 0.3261,  0.3775,  0.3488, -0.3917, -0.7329,  0.3141,  0.3891,  0.0750,\n",
      "         -1.2700]])  46  tensor(False)\n",
      "tensor([[ 0.3261,  0.2775,  0.3488, -0.3917, -0.7329,  0.3141,  0.2891,  0.0750,\n",
      "         -1.3700]])  47  tensor(False)\n",
      "tensor([[ 0.3261,  0.1775,  0.3488, -0.4917, -0.8329,  0.3141,  0.2891,  0.0750,\n",
      "         -1.3700]])  48  tensor(False)\n",
      "tensor([[ 0.2261,  0.1775,  0.3488, -0.4917, -0.8329,  0.3141,  0.1891,  0.0750,\n",
      "         -1.4700]])  49  tensor(False)\n",
      "tensor([[ 0.2261,  0.1775,  0.2488, -0.5917, -0.9329,  0.3141,  0.1891,  0.0750,\n",
      "         -1.4700]])  50  tensor(False)\n",
      "tensor([[ 0.2261,  0.1775,  0.2488, -0.5917, -1.0329,  0.2141,  0.0891,  0.0750,\n",
      "         -1.4700]])  51  tensor(False)\n",
      "tensor([[ 0.2261,  0.1775,  0.1488, -0.5917, -1.1329,  0.2141, -0.0109,  0.0750,\n",
      "         -1.4700]])  52  tensor(False)\n",
      "tensor([[ 0.2261,  0.1775,  0.1488, -0.5917, -1.1329,  0.1141, -0.1109,  0.0750,\n",
      "         -1.5700]])  53  tensor(False)\n",
      "tensor([[ 0.2261,  0.0775,  0.1488, -0.5917, -1.2329,  0.1141, -0.1109, -0.0250,\n",
      "         -1.5700]])  54  tensor(False)\n",
      "tensor([[ 0.1261,  0.0775,  0.1488, -0.5917, -1.2329,  0.1141, -0.1109, -0.1250,\n",
      "         -1.6700]])  55  tensor(False)\n",
      "tensor([[ 0.1261,  0.0775,  0.1488, -0.6917, -1.3329,  0.0141, -0.1109, -0.1250,\n",
      "         -1.6700]])  56  tensor(False)\n",
      "tensor([[ 0.1261,  0.0775,  0.0488, -0.6917, -1.4329,  0.0141, -0.2109, -0.1250,\n",
      "         -1.6700]])  57  tensor(False)\n",
      "tensor([[ 0.1261, -0.0225,  0.0488, -0.7917, -1.4329,  0.0141, -0.2109, -0.1250,\n",
      "         -1.7700]])  58  tensor(False)\n",
      "tensor([[ 0.0261, -0.0225,  0.0488, -0.7917, -1.5329,  0.0141, -0.3109, -0.1250,\n",
      "         -1.7700]])  59  tensor(False)\n",
      "tensor([[ 0.0261, -0.0225, -0.0512, -0.7917, -1.5329, -0.0859, -0.3109, -0.1250,\n",
      "         -1.8700]])  60  tensor(False)\n",
      "tensor([[-0.0739, -0.0225, -0.0512, -0.7917, -1.5329, -0.0859, -0.3109, -0.2250,\n",
      "         -1.9700]])  61  tensor(True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "692.7479248046875"
     },
     "execution_count": 218,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T09:11:16.116104200Z",
     "start_time": "2023-09-01T09:11:16.038100400Z"
    }
   },
   "id": "9c4e49ce22f9f053"
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T09:11:16.127111600Z",
     "start_time": "2023-09-01T09:11:16.110101100Z"
    }
   },
   "id": "7a3acb85b94d3e07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
