{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-05T03:33:38.173300100Z",
     "start_time": "2023-09-05T03:33:38.142403500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'tensor([[1.0408, 1.2495, 1.7930, 1.1497, 1.1469, 1.1127, 1.7079, 1.5717, 1.5242]])  步骤:1  游戏是否结束:0'"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "Carbin_Count = 9  # \n",
    "\n",
    "\n",
    "#定义环境\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # 每个船舱的重量\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.state = torch.FloatTensor(np.random.uniform(1, 2, (1, Carbin_Count)))\n",
    "        return self.state\n",
    "\n",
    "    def is_balance(self):\n",
    "        def _get_weight(num):\n",
    "            if num % 2 != 0:\n",
    "                return [i - (num - 1) // 2 for i in range(num)]\n",
    "            return [-i for i in range(num // 2, 0, -1)] + list(range(1, num // 2 + 1))\n",
    "\n",
    "        weight_list = _get_weight(Carbin_Count)\n",
    "        count = 0\n",
    "        for i in range(Carbin_Count):\n",
    "            count += weight_list[i] * self.state[0][i]\n",
    "        return abs(count) < 5\n",
    "\n",
    "    def is_over(self, action: torch.Tensor = []):\n",
    "        def is_fault(action: torch.Tensor):\n",
    "            last_action = -1\n",
    "            for i in action:\n",
    "                index = int(i)  # 映射到货轮\n",
    "                if index == 0:\n",
    "                    continue\n",
    "                if index < last_action:\n",
    "                    return True\n",
    "                last_action = index\n",
    "            return False\n",
    "\n",
    "        if is_fault(action) or not self.is_balance():\n",
    "            return -1\n",
    "        # 判断state每一项是否小于0\n",
    "        if torch.all(self.state < 0):\n",
    "            return 1\n",
    "        return 0\n",
    "\n",
    "    def get_state_reword(self, action: torch.Tensor):\n",
    "        # 当前状态的分数,比如为负数了还有人来卸载就应该扣分\n",
    "        reword = 0\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            if index == 0:\n",
    "                reword -= 3\n",
    "                continue\n",
    "            # 动起来的奖励\n",
    "            reword += 1 * 5\n",
    "            if self.state[0][index - 1] < 0:\n",
    "                reword += 5 * (self.state[0][index - 1] * 13)\n",
    "        is_over = self.is_over(action)\n",
    "        if is_over == 1:\n",
    "            reword += 1200\n",
    "        elif is_over == -1:\n",
    "            reword -= 200\n",
    "        return reword\n",
    "\n",
    "    def step(self, action):\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            if index == 0:\n",
    "                continue\n",
    "            self.state[0][index - 1] -= 0.1\n",
    "        self.time_spend += 1\n",
    "        reword = self.get_state_reword(action)\n",
    "        return self.state, reword, self.is_over(action) != 0, None\n",
    "\n",
    "\n",
    "class MyWrapper:\n",
    "    N = 3  # 港机数量\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(action)\n",
    "        state, reward, terminated, info = self.env.step(action)\n",
    "        over = terminated\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n > 200:\n",
    "            over = True\n",
    "        return state, reward, over\n",
    "\n",
    "    def to_show_state(self):\n",
    "        return '%s  步骤:%s  游戏是否结束:%s' % (str(self.env.state), str(self.env.time_spend), str(self.env.is_over()))\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "env.step(torch.full((1, 1), 0.5, dtype=torch.float32))\n",
    "env.to_show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[<__main__.A2C at 0x12cb6cbc040>,\n <__main__.A2C at 0x12cb6cbcbb0>,\n <__main__.A2C at 0x12caed455b0>]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class A2C:\n",
    "\n",
    "    def __init__(self, model_actor, model_critic, model_critic_delay,\n",
    "                 optimizer_actor, optimizer_critic):\n",
    "        self.model_actor = model_actor\n",
    "        self.model_critic = model_critic\n",
    "        self.model_critic_delay = model_critic_delay\n",
    "        self.optimizer_actor = optimizer_actor\n",
    "        self.optimizer_critic = optimizer_critic\n",
    "\n",
    "        self.model_critic_delay.load_state_dict(self.model_critic.state_dict())\n",
    "        self.requires_grad(self.model_critic_delay, False)\n",
    "\n",
    "    def soft_update(self, _from, _to):\n",
    "        for _from, _to in zip(_from.parameters(), _to.parameters()):\n",
    "            value = _to.data * 0.99 + _from.data * 0.01\n",
    "            _to.data.copy_(value)\n",
    "\n",
    "    def requires_grad(self, model, value):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(value)\n",
    "\n",
    "    def train_critic(self, state, reward, next_state, over):\n",
    "        self.requires_grad(self.model_critic, True)\n",
    "        self.requires_grad(self.model_actor, False)\n",
    "\n",
    "        #计算values和targets\n",
    "        value = self.model_critic(state)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target = self.model_critic_delay(next_state)\n",
    "        target = target * 0.99 * (1 - over) + reward\n",
    "        # print('xxxx', value.size(), target.size(), reward.size())\n",
    "        #时序差分误差,也就是tdloss\n",
    "        loss = torch.nn.functional.mse_loss(value, target)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        self.soft_update(self.model_critic, self.model_critic_delay)\n",
    "\n",
    "        #减去value相当于去基线\n",
    "        return (target - value).detach()\n",
    "\n",
    "    def train_actor(self, state, action, value):\n",
    "        self.requires_grad(self.model_critic, False)\n",
    "        self.requires_grad(self.model_actor, True)\n",
    "\n",
    "        #重新计算动作的概率\n",
    "        prob = self.model_actor(state)\n",
    "        prob = prob.gather(dim=1, index=action)\n",
    "\n",
    "        #根据策略梯度算法的导函数实现\n",
    "        #函数中的Q(state,action),这里使用critic模型估算\n",
    "        prob = (prob + 1e-8).log() * value\n",
    "        loss = -prob.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "model_actor = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, Carbin_Count + 1),\n",
    "        torch.nn.Softmax(dim=1),\n",
    "    ) for _ in range(env.N)\n",
    "]\n",
    "\n",
    "model_critic, model_critic_delay = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(64, 1),\n",
    "    ) for _ in range(2)\n",
    "]\n",
    "\n",
    "optimizer_actor = [\n",
    "    torch.optim.Adam(model_actor[i].parameters(), lr=1e-3)\n",
    "    for i in range(env.N)\n",
    "]\n",
    "optimizer_critic = torch.optim.Adam(model_critic.parameters(), lr=5e-3)\n",
    "\n",
    "a2c = [\n",
    "    A2C(model_actor[i], model_critic, model_critic_delay, optimizer_actor[i],\n",
    "        optimizer_critic) for i in range(env.N)\n",
    "]\n",
    "# x = torch.FloatTensor([1,2,3,4,6,7,8,8,4]).resize(1,Carbin_Count)\n",
    "# print(model_actor[0](x))\n",
    "model_actor = None\n",
    "model_critic = None\n",
    "model_critic_delay = None\n",
    "optimizer_actor = None\n",
    "optimizer_critic = None\n",
    "\n",
    "a2c\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T03:33:38.240383400Z",
     "start_time": "2023-09-05T03:33:38.174296700Z"
    }
   },
   "id": "955e583b006b55e6"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "(-185.0, torch.Size([1, 1, 9]), torch.Size([1, 1, 1]), torch.Size([1, 3, 1]))"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        a = []\n",
    "        for i in range(env.N):\n",
    "            #计算动作\n",
    "            prob = a2c[i].model_actor(torch.FloatTensor(s).reshape(\n",
    "                1, -1))[0].tolist()\n",
    "            # print(s, prob)\n",
    "            a.append(random.choices(range(Carbin_Count + 1), weights=prob, k=1)[0])\n",
    "\n",
    "        #执行动作\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            print(a, env.to_show_state())\n",
    "    # print(state[0])\n",
    "    # print(type(state), len(state))\n",
    "    state = torch.tensor([item.numpy() for item in state])\n",
    "    action = torch.LongTensor(action).unsqueeze(-1)\n",
    "    reward = torch.FloatTensor(reward).unsqueeze(-1).unsqueeze(-1)\n",
    "    next_state = torch.tensor([item.numpy() for item in next_state])\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum, state.size(), reward.size(), action.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T03:33:38.241424600Z",
     "start_time": "2023-09-05T03:33:38.202203300Z"
    }
   },
   "id": "e01ea439facc25cf"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -30.754661560058594 1009.3025482177734\n",
      "250 -33.201744079589844 1228.970233154297\n",
      "500 -21.294675827026367 1175.537042236328\n",
      "750 12.735350608825684 890.4701263427735\n",
      "1000 -59.23042297363281 1099.33642578125\n",
      "1250 -34.20117950439453 1165.6182739257813\n",
      "1500 -25.605192184448242 1345.0637756347655\n",
      "1750 -42.75160598754883 1330.5882995605468\n",
      "2000 -55.612022399902344 1258.7829162597657\n",
      "2250 -29.451799392700195 1372.3510192871095\n",
      "2500 -56.7697868347168 1264.2888061523438\n",
      "2750 -21.67510986328125 1224.0494140625\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    #训练N局\n",
    "    for epoch in range(3_000):\n",
    "        state, action, reward, next_state, over, _ = play()\n",
    "\n",
    "        #合并部分字段\n",
    "        state_c = state.flatten(start_dim=1)\n",
    "        reward_c = reward.sum(dim=1)\n",
    "        next_state_c = next_state.flatten(start_dim=1)\n",
    "\n",
    "        for i in range(env.N):\n",
    "            value = a2c[i].train_critic(state_c, reward_c, next_state_c, over)\n",
    "            loss = a2c[i].train_actor(state_c, action[:, i], value)\n",
    "\n",
    "        if epoch % 250 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T03:43:08.571302900Z",
     "start_time": "2023-09-05T03:39:15.064571400Z"
    }
   },
   "id": "660f338bb74d7ff2"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 5, 9] tensor([[1.2603, 1.2243, 1.9355, 1.5128, 1.7008, 1.9535, 1.7696, 1.2639, 1.5422]])  步骤:1  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[1.1603, 1.2243, 1.9355, 1.5128, 1.6008, 1.9535, 1.7696, 1.2639, 1.4422]])  步骤:2  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[1.0603, 1.2243, 1.9355, 1.5128, 1.5008, 1.9535, 1.7696, 1.2639, 1.3422]])  步骤:3  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.9603, 1.2243, 1.9355, 1.5128, 1.4008, 1.9535, 1.7696, 1.2639, 1.2422]])  步骤:4  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.8603, 1.2243, 1.9355, 1.5128, 1.3008, 1.9535, 1.7696, 1.2639, 1.1422]])  步骤:5  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.7603, 1.2243, 1.9355, 1.5128, 1.2008, 1.9535, 1.7696, 1.2639, 1.0422]])  步骤:6  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.6603, 1.2243, 1.9355, 1.5128, 1.1008, 1.9535, 1.7696, 1.2639, 0.9422]])  步骤:7  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.5603, 1.2243, 1.9355, 1.5128, 1.0008, 1.9535, 1.7696, 1.2639, 0.8422]])  步骤:8  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.4603, 1.2243, 1.9355, 1.5128, 0.9008, 1.9535, 1.7696, 1.2639, 0.7422]])  步骤:9  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.3603, 1.2243, 1.9355, 1.5128, 0.8008, 1.9535, 1.7696, 1.2639, 0.6422]])  步骤:10  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.2603, 1.2243, 1.9355, 1.5128, 0.7008, 1.9535, 1.7696, 1.2639, 0.5422]])  步骤:11  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.1603, 1.2243, 1.9355, 1.5128, 0.6008, 1.9535, 1.7696, 1.2639, 0.4422]])  步骤:12  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[0.0603, 1.2243, 1.9355, 1.5128, 0.5008, 1.9535, 1.7696, 1.2639, 0.3422]])  步骤:13  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[-0.0397,  1.2243,  1.9355,  1.5128,  0.4008,  1.9535,  1.7696,  1.2639,\n",
      "          0.2422]])  步骤:14  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[-0.1397,  1.2243,  1.9355,  1.5128,  0.3008,  1.9535,  1.7696,  1.2639,\n",
      "          0.1422]])  步骤:15  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[-0.2397,  1.2243,  1.9355,  1.5128,  0.2008,  1.9535,  1.7696,  1.2639,\n",
      "          0.0422]])  步骤:16  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[-0.3397,  1.2243,  1.9355,  1.5128,  0.1008,  1.9535,  1.7696,  1.2639,\n",
      "         -0.0578]])  步骤:17  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[-4.3970e-01,  1.2243e+00,  1.9355e+00,  1.5128e+00,  8.0957e-04,\n",
      "          1.9535e+00,  1.7696e+00,  1.2639e+00, -1.5775e-01]])  步骤:18  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[-0.5397,  1.2243,  1.9355,  1.5128, -0.0992,  1.9535,  1.7696,  1.2639,\n",
      "         -0.2578]])  步骤:19  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[-0.6397,  1.2243,  1.9355,  1.5128, -0.1992,  1.9535,  1.7696,  1.2639,\n",
      "         -0.3578]])  步骤:20  游戏是否结束:0\n",
      "[1, 5, 9] tensor([[-0.7397,  1.2243,  1.9355,  1.5128, -0.2992,  1.9535,  1.7696,  1.2639,\n",
      "         -0.4578]])  步骤:21  游戏是否结束:0\n",
      "[2, 5, 0] tensor([[-0.7397,  1.1243,  1.9355,  1.5128, -0.3992,  1.9535,  1.7696,  1.2639,\n",
      "         -0.4578]])  步骤:22  游戏是否结束:0\n",
      "[0, 7, 9] tensor([[-0.7397,  1.1243,  1.9355,  1.5128, -0.3992,  1.9535,  1.6696,  1.2639,\n",
      "         -0.5578]])  步骤:23  游戏是否结束:0\n",
      "[0, 8, 0] tensor([[-0.7397,  1.1243,  1.9355,  1.5128, -0.3992,  1.9535,  1.6696,  1.1639,\n",
      "         -0.5578]])  步骤:24  游戏是否结束:0\n",
      "[2, 7, 0] tensor([[-0.7397,  1.0243,  1.9355,  1.5128, -0.3992,  1.9535,  1.5696,  1.1639,\n",
      "         -0.5578]])  步骤:25  游戏是否结束:0\n",
      "[0, 7, 0] tensor([[-0.7397,  1.0243,  1.9355,  1.5128, -0.3992,  1.9535,  1.4696,  1.1639,\n",
      "         -0.5578]])  步骤:26  游戏是否结束:0\n",
      "[3, 7, 0] tensor([[-0.7397,  1.0243,  1.8355,  1.5128, -0.3992,  1.9535,  1.3696,  1.1639,\n",
      "         -0.5578]])  步骤:27  游戏是否结束:0\n",
      "[0, 6, 0] tensor([[-0.7397,  1.0243,  1.8355,  1.5128, -0.3992,  1.8535,  1.3696,  1.1639,\n",
      "         -0.5578]])  步骤:28  游戏是否结束:0\n",
      "[2, 7, 0] tensor([[-0.7397,  0.9243,  1.8355,  1.5128, -0.3992,  1.8535,  1.2696,  1.1639,\n",
      "         -0.5578]])  步骤:29  游戏是否结束:0\n",
      "[3, 6, 0] tensor([[-0.7397,  0.9243,  1.7355,  1.5128, -0.3992,  1.7535,  1.2696,  1.1639,\n",
      "         -0.5578]])  步骤:30  游戏是否结束:0\n",
      "[0, 8, 0] tensor([[-0.7397,  0.9243,  1.7355,  1.5128, -0.3992,  1.7535,  1.2696,  1.0639,\n",
      "         -0.5578]])  步骤:31  游戏是否结束:0\n",
      "[0, 6, 0] tensor([[-0.7397,  0.9243,  1.7355,  1.5128, -0.3992,  1.6535,  1.2696,  1.0639,\n",
      "         -0.5578]])  步骤:32  游戏是否结束:0\n",
      "[2, 8, 0] tensor([[-0.7397,  0.8243,  1.7355,  1.5128, -0.3992,  1.6535,  1.2696,  0.9639,\n",
      "         -0.5578]])  步骤:33  游戏是否结束:0\n",
      "[3, 6, 0] tensor([[-0.7397,  0.8243,  1.6355,  1.5128, -0.3992,  1.5535,  1.2696,  0.9639,\n",
      "         -0.5578]])  步骤:34  游戏是否结束:0\n",
      "[0, 4, 0] tensor([[-0.7397,  0.8243,  1.6355,  1.4128, -0.3992,  1.5535,  1.2696,  0.9639,\n",
      "         -0.5578]])  步骤:35  游戏是否结束:0\n",
      "[2, 7, 0] tensor([[-0.7397,  0.7243,  1.6355,  1.4128, -0.3992,  1.5535,  1.1696,  0.9639,\n",
      "         -0.5578]])  步骤:36  游戏是否结束:0\n",
      "[0, 7, 0] tensor([[-0.7397,  0.7243,  1.6355,  1.4128, -0.3992,  1.5535,  1.0696,  0.9639,\n",
      "         -0.5578]])  步骤:37  游戏是否结束:0\n",
      "[0, 6, 0] tensor([[-0.7397,  0.7243,  1.6355,  1.4128, -0.3992,  1.4535,  1.0696,  0.9639,\n",
      "         -0.5578]])  步骤:38  游戏是否结束:0\n",
      "[0, 8, 0] tensor([[-0.7397,  0.7243,  1.6355,  1.4128, -0.3992,  1.4535,  1.0696,  0.8639,\n",
      "         -0.5578]])  步骤:39  游戏是否结束:0\n",
      "[3, 4, 0] tensor([[-0.7397,  0.7243,  1.5355,  1.3128, -0.3992,  1.4535,  1.0696,  0.8639,\n",
      "         -0.5578]])  步骤:40  游戏是否结束:0\n",
      "[3, 4, 0] tensor([[-0.7397,  0.7243,  1.4355,  1.2128, -0.3992,  1.4535,  1.0696,  0.8639,\n",
      "         -0.5578]])  步骤:41  游戏是否结束:0\n",
      "[0, 7, 0] tensor([[-0.7397,  0.7243,  1.4355,  1.2128, -0.3992,  1.4535,  0.9696,  0.8639,\n",
      "         -0.5578]])  步骤:42  游戏是否结束:0\n",
      "[3, 6, 0] tensor([[-0.7397,  0.7243,  1.3355,  1.2128, -0.3992,  1.3535,  0.9696,  0.8639,\n",
      "         -0.5578]])  步骤:43  游戏是否结束:0\n",
      "[3, 6, 0] tensor([[-0.7397,  0.7243,  1.2355,  1.2128, -0.3992,  1.2535,  0.9696,  0.8639,\n",
      "         -0.5578]])  步骤:44  游戏是否结束:0\n",
      "[3, 7, 0] tensor([[-0.7397,  0.7243,  1.1355,  1.2128, -0.3992,  1.2535,  0.8696,  0.8639,\n",
      "         -0.5578]])  步骤:45  游戏是否结束:0\n",
      "[2, 6, 0] tensor([[-0.7397,  0.6243,  1.1355,  1.2128, -0.3992,  1.1535,  0.8696,  0.8639,\n",
      "         -0.5578]])  步骤:46  游戏是否结束:0\n",
      "[3, 8, 0] tensor([[-0.7397,  0.6243,  1.0355,  1.2128, -0.3992,  1.1535,  0.8696,  0.7639,\n",
      "         -0.5578]])  步骤:47  游戏是否结束:0\n",
      "[2, 8, 0] tensor([[-0.7397,  0.5243,  1.0355,  1.2128, -0.3992,  1.1535,  0.8696,  0.6639,\n",
      "         -0.5578]])  步骤:48  游戏是否结束:0\n",
      "[0, 6, 0] tensor([[-0.7397,  0.5243,  1.0355,  1.2128, -0.3992,  1.0535,  0.8696,  0.6639,\n",
      "         -0.5578]])  步骤:49  游戏是否结束:0\n",
      "[0, 7, 0] tensor([[-0.7397,  0.5243,  1.0355,  1.2128, -0.3992,  1.0535,  0.7696,  0.6639,\n",
      "         -0.5578]])  步骤:50  游戏是否结束:0\n",
      "[2, 8, 0] tensor([[-0.7397,  0.4243,  1.0355,  1.2128, -0.3992,  1.0535,  0.7696,  0.5639,\n",
      "         -0.5578]])  步骤:51  游戏是否结束:0\n",
      "[0, 4, 0] tensor([[-0.7397,  0.4243,  1.0355,  1.1128, -0.3992,  1.0535,  0.7696,  0.5639,\n",
      "         -0.5578]])  步骤:52  游戏是否结束:0\n",
      "[2, 8, 0] tensor([[-0.7397,  0.3243,  1.0355,  1.1128, -0.3992,  1.0535,  0.7696,  0.4639,\n",
      "         -0.5578]])  步骤:53  游戏是否结束:0\n",
      "[0, 7, 0] tensor([[-0.7397,  0.3243,  1.0355,  1.1128, -0.3992,  1.0535,  0.6696,  0.4639,\n",
      "         -0.5578]])  步骤:54  游戏是否结束:0\n",
      "[0, 6, 0] tensor([[-0.7397,  0.3243,  1.0355,  1.1128, -0.3992,  0.9535,  0.6696,  0.4639,\n",
      "         -0.5578]])  步骤:55  游戏是否结束:0\n",
      "[3, 3, 0] tensor([[-0.7397,  0.3243,  0.8355,  1.1128, -0.3992,  0.9535,  0.6696,  0.4639,\n",
      "         -0.5578]])  步骤:56  游戏是否结束:0\n",
      "[0, 4, 0] tensor([[-0.7397,  0.3243,  0.8355,  1.0128, -0.3992,  0.9535,  0.6696,  0.4639,\n",
      "         -0.5578]])  步骤:57  游戏是否结束:0\n",
      "[3, 6, 0] tensor([[-0.7397,  0.3243,  0.7355,  1.0128, -0.3992,  0.8535,  0.6696,  0.4639,\n",
      "         -0.5578]])  步骤:58  游戏是否结束:0\n",
      "[0, 4, 0] tensor([[-0.7397,  0.3243,  0.7355,  0.9128, -0.3992,  0.8535,  0.6696,  0.4639,\n",
      "         -0.5578]])  步骤:59  游戏是否结束:0\n",
      "[0, 6, 0] tensor([[-0.7397,  0.3243,  0.7355,  0.9128, -0.3992,  0.7535,  0.6696,  0.4639,\n",
      "         -0.5578]])  步骤:60  游戏是否结束:0\n",
      "[2, 6, 0] tensor([[-0.7397,  0.2243,  0.7355,  0.9128, -0.3992,  0.6535,  0.6696,  0.4639,\n",
      "         -0.5578]])  步骤:61  游戏是否结束:0\n",
      "[0, 4, 0] tensor([[-0.7397,  0.2243,  0.7355,  0.8128, -0.3992,  0.6535,  0.6696,  0.4639,\n",
      "         -0.5578]])  步骤:62  游戏是否结束:0\n",
      "[3, 7, 0] tensor([[-0.7397,  0.2243,  0.6355,  0.8128, -0.3992,  0.6535,  0.5696,  0.4639,\n",
      "         -0.5578]])  步骤:63  游戏是否结束:0\n",
      "[2, 4, 0] tensor([[-0.7397,  0.1243,  0.6355,  0.7128, -0.3992,  0.6535,  0.5696,  0.4639,\n",
      "         -0.5578]])  步骤:64  游戏是否结束:0\n",
      "[3, 6, 0] tensor([[-0.7397,  0.1243,  0.5355,  0.7128, -0.3992,  0.5535,  0.5696,  0.4639,\n",
      "         -0.5578]])  步骤:65  游戏是否结束:0\n",
      "[0, 4, 0] tensor([[-0.7397,  0.1243,  0.5355,  0.6128, -0.3992,  0.5535,  0.5696,  0.4639,\n",
      "         -0.5578]])  步骤:66  游戏是否结束:0\n",
      "[3, 8, 0] tensor([[-0.7397,  0.1243,  0.4355,  0.6128, -0.3992,  0.5535,  0.5696,  0.3639,\n",
      "         -0.5578]])  步骤:67  游戏是否结束:0\n",
      "[3, 7, 0] tensor([[-0.7397,  0.1243,  0.3355,  0.6128, -0.3992,  0.5535,  0.4696,  0.3639,\n",
      "         -0.5578]])  步骤:68  游戏是否结束:0\n",
      "[3, 6, 0] tensor([[-0.7397,  0.1243,  0.2355,  0.6128, -0.3992,  0.4535,  0.4696,  0.3639,\n",
      "         -0.5578]])  步骤:69  游戏是否结束:0\n",
      "[0, 7, 0] tensor([[-0.7397,  0.1243,  0.2355,  0.6128, -0.3992,  0.4535,  0.3696,  0.3639,\n",
      "         -0.5578]])  步骤:70  游戏是否结束:0\n",
      "[0, 6, 0] tensor([[-0.7397,  0.1243,  0.2355,  0.6128, -0.3992,  0.3535,  0.3696,  0.3639,\n",
      "         -0.5578]])  步骤:71  游戏是否结束:0\n",
      "[2, 4, 0] tensor([[-0.7397,  0.0243,  0.2355,  0.5128, -0.3992,  0.3535,  0.3696,  0.3639,\n",
      "         -0.5578]])  步骤:72  游戏是否结束:0\n",
      "[3, 4, 0] tensor([[-0.7397,  0.0243,  0.1355,  0.4128, -0.3992,  0.3535,  0.3696,  0.3639,\n",
      "         -0.5578]])  步骤:73  游戏是否结束:0\n",
      "[0, 7, 0] tensor([[-0.7397,  0.0243,  0.1355,  0.4128, -0.3992,  0.3535,  0.2696,  0.3639,\n",
      "         -0.5578]])  步骤:74  游戏是否结束:0\n",
      "[2, 4, 0] tensor([[-0.7397, -0.0757,  0.1355,  0.3128, -0.3992,  0.3535,  0.2696,  0.3639,\n",
      "         -0.5578]])  步骤:75  游戏是否结束:0\n",
      "[3, 8, 0] tensor([[-0.7397, -0.0757,  0.0355,  0.3128, -0.3992,  0.3535,  0.2696,  0.2639,\n",
      "         -0.5578]])  步骤:76  游戏是否结束:0\n",
      "[3, 6, 0] tensor([[-0.7397, -0.0757, -0.0645,  0.3128, -0.3992,  0.2535,  0.2696,  0.2639,\n",
      "         -0.5578]])  步骤:77  游戏是否结束:0\n",
      "[0, 4, 0] tensor([[-0.7397, -0.0757, -0.0645,  0.2128, -0.3992,  0.2535,  0.2696,  0.2639,\n",
      "         -0.5578]])  步骤:78  游戏是否结束:0\n",
      "[3, 8, 0] tensor([[-0.7397, -0.0757, -0.1645,  0.2128, -0.3992,  0.2535,  0.2696,  0.1639,\n",
      "         -0.5578]])  步骤:79  游戏是否结束:0\n",
      "[2, 7, 0] tensor([[-0.7397, -0.1757, -0.1645,  0.2128, -0.3992,  0.2535,  0.1696,  0.1639,\n",
      "         -0.5578]])  步骤:80  游戏是否结束:0\n",
      "[3, 4, 0] tensor([[-0.7397, -0.1757, -0.2645,  0.1128, -0.3992,  0.2535,  0.1696,  0.1639,\n",
      "         -0.5578]])  步骤:81  游戏是否结束:0\n",
      "[0, 6, 0] tensor([[-0.7397, -0.1757, -0.2645,  0.1128, -0.3992,  0.1535,  0.1696,  0.1639,\n",
      "         -0.5578]])  步骤:82  游戏是否结束:0\n",
      "[0, 8, 0] tensor([[-0.7397, -0.1757, -0.2645,  0.1128, -0.3992,  0.1535,  0.1696,  0.0639,\n",
      "         -0.5578]])  步骤:83  游戏是否结束:0\n",
      "[3, 6, 0] tensor([[-0.7397, -0.1757, -0.3645,  0.1128, -0.3992,  0.0535,  0.1696,  0.0639,\n",
      "         -0.5578]])  步骤:84  游戏是否结束:0\n",
      "[0, 7, 0] tensor([[-0.7397, -0.1757, -0.3645,  0.1128, -0.3992,  0.0535,  0.0696,  0.0639,\n",
      "         -0.5578]])  步骤:85  游戏是否结束:0\n",
      "[2, 8, 0] tensor([[-0.7397, -0.2757, -0.3645,  0.1128, -0.3992,  0.0535,  0.0696, -0.0361,\n",
      "         -0.5578]])  步骤:86  游戏是否结束:0\n",
      "[0, 4, 0] tensor([[-0.7397, -0.2757, -0.3645,  0.0128, -0.3992,  0.0535,  0.0696, -0.0361,\n",
      "         -0.5578]])  步骤:87  游戏是否结束:0\n",
      "[0, 6, 0] tensor([[-0.7397, -0.2757, -0.3645,  0.0128, -0.3992, -0.0465,  0.0696, -0.0361,\n",
      "         -0.5578]])  步骤:88  游戏是否结束:0\n",
      "[3, 4, 0] tensor([[-0.7397, -0.2757, -0.4645, -0.0872, -0.3992, -0.0465,  0.0696, -0.0361,\n",
      "         -0.5578]])  步骤:89  游戏是否结束:0\n",
      "[0, 7, 0] tensor([[-0.7397, -0.2757, -0.4645, -0.0872, -0.3992, -0.0465, -0.0304, -0.0361,\n",
      "         -0.5578]])  步骤:90  游戏是否结束:1\n"
     ]
    },
    {
     "data": {
      "text/plain": "1237.306396484375"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T03:37:32.799648600Z",
     "start_time": "2023-09-05T03:37:32.671813800Z"
    }
   },
   "id": "9c4e49ce22f9f053"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-05T03:37:32.841513Z",
     "start_time": "2023-09-05T03:37:32.797654800Z"
    }
   },
   "id": "7a3acb85b94d3e07"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
