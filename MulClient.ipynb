{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-01T05:15:43.085644400Z",
     "start_time": "2023-09-01T05:15:43.048722500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'tensor([[-0.8474,  0.7139,  0.8216,  2.1499,  0.1014, -0.7658,  1.8822,  2.3720,\\n          0.0216]])  1  tensor(False)'"
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from random import randint\n",
    "import torch\n",
    "\n",
    "Carbin_Count = 9  # \n",
    "\n",
    "\n",
    "#定义环境\n",
    "class Environment:\n",
    "    def __init__(self):\n",
    "        self.state = None\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        # 每个船舱的重量\n",
    "        self.time_spend = 0  # 使用的总共时间 要惩罚时间的\n",
    "        self.state = torch.randn(1, Carbin_Count) + torch.full((1, Carbin_Count), 1).resize(1,\n",
    "                                                                                            Carbin_Count)\n",
    "        return self.state\n",
    "\n",
    "    def is_over(self):\n",
    "        # 判断state每一项是否小于0\n",
    "        return torch.all(self.state < 0)\n",
    "\n",
    "    def get_state_reword(self, action: torch.Tensor):\n",
    "        # 当前状态的分数,比如为负数了还有人来卸载就应该扣分\n",
    "        reword = 0\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            if index == 0:\n",
    "                continue\n",
    "            # 动起来的奖励\n",
    "            reword += 0.5\n",
    "            if self.state[0][index - 1] < 0:\n",
    "                reword -= 1\n",
    "        if self.is_over():\n",
    "            # 惩罚时间 \n",
    "            reword += 1000\n",
    "        return reword\n",
    "\n",
    "    def step(self, action):\n",
    "        for i in action:\n",
    "            index = int(i)  # 映射到货轮\n",
    "            if index == 0:\n",
    "                continue\n",
    "            self.state[0][index-1] -= 0.1\n",
    "        self.time_spend += 1\n",
    "        reword = self.get_state_reword(action)\n",
    "        return self.state, reword, self.is_over(), None\n",
    "\n",
    "\n",
    "class MyWrapper:\n",
    "    N = 3  # 港机数量\n",
    "\n",
    "    def __init__(self):\n",
    "        self.env = Environment()\n",
    "        self.step_n = 0\n",
    "\n",
    "    def reset(self):\n",
    "        state = self.env.reset()\n",
    "        self.step_n = 0\n",
    "        return state\n",
    "\n",
    "    def step(self, action):\n",
    "        # print(action)\n",
    "        state, reward, terminated, info = self.env.step(action)\n",
    "        over = terminated\n",
    "        #限制最大步数\n",
    "        self.step_n += 1\n",
    "        if self.step_n > 200:\n",
    "            over = True\n",
    "        return state, reward, over\n",
    "\n",
    "    def to_show_state(self):\n",
    "        return '%s  %s  %s' % (str(self.env.state), str(self.env.time_spend), str(self.env.is_over()))\n",
    "\n",
    "\n",
    "env = MyWrapper()\n",
    "\n",
    "env.reset()\n",
    "env.step(torch.full((1, 1), 0.5, dtype=torch.float32))\n",
    "env.to_show_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "outputs": [
    {
     "data": {
      "text/plain": "[<__main__.A2C at 0x1690fa50c10>,\n <__main__.A2C at 0x1690fa7a550>,\n <__main__.A2C at 0x1690fa7afd0>]"
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "class A2C:\n",
    "\n",
    "    def __init__(self, model_actor, model_critic, model_critic_delay,\n",
    "                 optimizer_actor, optimizer_critic):\n",
    "        self.model_actor = model_actor\n",
    "        self.model_critic = model_critic\n",
    "        self.model_critic_delay = model_critic_delay\n",
    "        self.optimizer_actor = optimizer_actor\n",
    "        self.optimizer_critic = optimizer_critic\n",
    "\n",
    "        self.model_critic_delay.load_state_dict(self.model_critic.state_dict())\n",
    "        self.requires_grad(self.model_critic_delay, False)\n",
    "\n",
    "    def soft_update(self, _from, _to):\n",
    "        for _from, _to in zip(_from.parameters(), _to.parameters()):\n",
    "            value = _to.data * 0.99 + _from.data * 0.01\n",
    "            _to.data.copy_(value)\n",
    "\n",
    "    def requires_grad(self, model, value):\n",
    "        for param in model.parameters():\n",
    "            param.requires_grad_(value)\n",
    "\n",
    "    def train_critic(self, state, reward, next_state, over):\n",
    "        self.requires_grad(self.model_critic, True)\n",
    "        self.requires_grad(self.model_actor, False)\n",
    "\n",
    "        #计算values和targets\n",
    "        value = self.model_critic(state)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            target = self.model_critic_delay(next_state)\n",
    "        target = target * 0.99 * (1 - over) + reward\n",
    "        # print('xxxx', value.size(), target.size(), reward.size())\n",
    "        #时序差分误差,也就是tdloss\n",
    "        loss = torch.nn.functional.mse_loss(value, target)\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_critic.step()\n",
    "        self.optimizer_critic.zero_grad()\n",
    "        self.soft_update(self.model_critic, self.model_critic_delay)\n",
    "\n",
    "        #减去value相当于去基线\n",
    "        return (target - value).detach()\n",
    "\n",
    "    def train_actor(self, state, action, value):\n",
    "        self.requires_grad(self.model_critic, False)\n",
    "        self.requires_grad(self.model_actor, True)\n",
    "\n",
    "        #重新计算动作的概率\n",
    "        prob = self.model_actor(state)\n",
    "        prob = prob.gather(dim=1, index=action)\n",
    "\n",
    "        #根据策略梯度算法的导函数实现\n",
    "        #函数中的Q(state,action),这里使用critic模型估算\n",
    "        prob = (prob + 1e-8).log() * value\n",
    "        loss = -prob.mean()\n",
    "\n",
    "        loss.backward()\n",
    "        self.optimizer_actor.step()\n",
    "        self.optimizer_actor.zero_grad()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "\n",
    "model_actor = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, Carbin_Count + 1),\n",
    "        torch.nn.Softmax(dim=1),\n",
    "    ) for _ in range(env.N)\n",
    "]\n",
    "\n",
    "model_critic, model_critic_delay = [\n",
    "    torch.nn.Sequential(\n",
    "        torch.nn.Linear(9, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 6 * 64),\n",
    "        torch.nn.ReLU(),\n",
    "        torch.nn.Linear(6 * 64, 1),\n",
    "    ) for _ in range(2)\n",
    "]\n",
    "\n",
    "optimizer_actor = [\n",
    "    torch.optim.Adam(model_actor[i].parameters(), lr=1e-3)\n",
    "    for i in range(env.N)\n",
    "]\n",
    "optimizer_critic = torch.optim.Adam(model_critic.parameters(), lr=5e-3)\n",
    "\n",
    "a2c = [\n",
    "    A2C(model_actor[i], model_critic, model_critic_delay, optimizer_actor[i],\n",
    "        optimizer_critic) for i in range(env.N)\n",
    "]\n",
    "# x = torch.FloatTensor([1,2,3,4,6,7,8,8,4]).resize(1,Carbin_Count)\n",
    "# print(model_actor[0](x))\n",
    "model_actor = None\n",
    "model_critic = None\n",
    "model_critic_delay = None\n",
    "optimizer_actor = None\n",
    "optimizer_critic = None\n",
    "\n",
    "a2c\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T05:15:43.142640200Z",
     "start_time": "2023-09-01T05:15:43.080639800Z"
    }
   },
   "id": "955e583b006b55e6"
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "outputs": [
    {
     "data": {
      "text/plain": "(960.5,\n torch.Size([112, 1, 9]),\n torch.Size([112, 1, 1]),\n torch.Size([112, 3, 1]))"
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "\n",
    "#玩一局游戏并记录数据\n",
    "def play(show=False):\n",
    "    state = []\n",
    "    action = []\n",
    "    reward = []\n",
    "    next_state = []\n",
    "    over = []\n",
    "\n",
    "    s = env.reset()\n",
    "    o = False\n",
    "    while not o:\n",
    "        a = []\n",
    "        for i in range(env.N):\n",
    "            #计算动作\n",
    "            prob = a2c[i].model_actor(torch.FloatTensor(s).reshape(\n",
    "                1, -1))[0].tolist()\n",
    "            # print(s, prob)\n",
    "            a.append(random.choices(range(Carbin_Count + 1), weights=prob, k=1)[0])\n",
    "\n",
    "        #执行动作\n",
    "        ns, r, o = env.step(a)\n",
    "\n",
    "        state.append(s)\n",
    "        action.append(a)\n",
    "        reward.append(r)\n",
    "        next_state.append(ns)\n",
    "        over.append(o)\n",
    "\n",
    "        s = ns\n",
    "\n",
    "        if show:\n",
    "            print(env.to_show_state())\n",
    "    # print(state[0])\n",
    "    # print(type(state), len(state))\n",
    "    state = torch.tensor([item.numpy() for item in state])\n",
    "    action = torch.LongTensor(action).unsqueeze(-1)\n",
    "    reward = torch.FloatTensor(reward).unsqueeze(-1).unsqueeze(-1)\n",
    "    next_state = torch.tensor([item.numpy() for item in next_state])\n",
    "    over = torch.LongTensor(over).reshape(-1, 1)\n",
    "\n",
    "    return state, action, reward, next_state, over, reward.sum().item()\n",
    "\n",
    "\n",
    "state, action, reward, next_state, over, reward_sum = play()\n",
    "\n",
    "reward_sum, state.size(), reward.size(), action.size()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T05:15:43.199640400Z",
     "start_time": "2023-09-01T05:15:43.115639700Z"
    }
   },
   "id": "e01ea439facc25cf"
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 -1.8459137678146362 964.325\n",
      "250 4.482891082763672 62.275\n",
      "500 -8.162630081176758 168.875\n",
      "750 -0.0 552.225\n",
      "1000 -0.0 926.225\n",
      "1250 -0.0 935.85\n",
      "1500 0.05349911004304886 953.25\n",
      "1750 -0.0 954.375\n",
      "2000 -0.0 955.775\n",
      "2250 -0.0 968.625\n",
      "2500 -1.3746167421340942 971.75\n",
      "2750 -0.0 972.475\n"
     ]
    }
   ],
   "source": [
    "def train():\n",
    "    #训练N局\n",
    "    for epoch in range(3_000):\n",
    "        state, action, reward, next_state, over, _ = play()\n",
    "\n",
    "        #合并部分字段\n",
    "        state_c = state.flatten(start_dim=1)\n",
    "        reward_c = reward.sum(dim=1)\n",
    "        next_state_c = next_state.flatten(start_dim=1)\n",
    "\n",
    "        for i in range(env.N):\n",
    "            value = a2c[i].train_critic(state_c, reward_c, next_state_c, over)\n",
    "            loss = a2c[i].train_actor(state_c, action[:, i], value)\n",
    "\n",
    "        if epoch % 250 == 0:\n",
    "            test_result = sum([play()[-1] for _ in range(20)]) / 20\n",
    "            print(epoch, loss, test_result)\n",
    "\n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T05:20:47.673366Z",
     "start_time": "2023-09-01T05:15:43.202640800Z"
    }
   },
   "id": "660f338bb74d7ff2"
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0775,  0.1910,  0.3738,  2.0309, -1.8991,  1.0799,  0.9674,  0.3243,\n",
      "         -0.1416]])  1  tensor(False)\n",
      "tensor([[-0.0775,  0.0910,  0.3738,  1.9309, -1.9991,  1.0799,  0.9674,  0.3243,\n",
      "         -0.1416]])  2  tensor(False)\n",
      "tensor([[-0.0775, -0.0090,  0.2738,  1.9309, -2.0991,  1.0799,  0.9674,  0.3243,\n",
      "         -0.1416]])  3  tensor(False)\n",
      "tensor([[-0.0775, -0.1090,  0.2738,  1.8309, -2.1991,  1.0799,  0.9674,  0.3243,\n",
      "         -0.1416]])  4  tensor(False)\n",
      "tensor([[-0.0775, -0.2090,  0.2738,  1.7309, -2.2991,  1.0799,  0.9674,  0.3243,\n",
      "         -0.1416]])  5  tensor(False)\n",
      "tensor([[-0.0775, -0.3090,  0.2738,  1.7309, -2.3991,  0.9799,  0.9674,  0.3243,\n",
      "         -0.1416]])  6  tensor(False)\n",
      "tensor([[-0.0775, -0.4090,  0.2738,  1.6309, -2.4991,  0.9799,  0.9674,  0.3243,\n",
      "         -0.1416]])  7  tensor(False)\n",
      "tensor([[-0.0775, -0.5090,  0.2738,  1.5309, -2.5991,  0.9799,  0.9674,  0.3243,\n",
      "         -0.1416]])  8  tensor(False)\n",
      "tensor([[-0.0775, -0.6090,  0.2738,  1.4309, -2.6991,  0.9799,  0.9674,  0.3243,\n",
      "         -0.1416]])  9  tensor(False)\n",
      "tensor([[-0.0775, -0.7090,  0.2738,  1.4309, -2.7991,  0.8799,  0.9674,  0.3243,\n",
      "         -0.1416]])  10  tensor(False)\n",
      "tensor([[-0.0775, -0.8090,  0.1738,  1.4309, -2.8991,  0.8799,  0.9674,  0.3243,\n",
      "         -0.1416]])  11  tensor(False)\n",
      "tensor([[-0.0775, -0.9090,  0.1738,  1.3309, -2.9991,  0.8799,  0.9674,  0.3243,\n",
      "         -0.1416]])  12  tensor(False)\n",
      "tensor([[-0.0775, -1.0090,  0.1738,  1.2309, -3.0991,  0.8799,  0.9674,  0.3243,\n",
      "         -0.1416]])  13  tensor(False)\n",
      "tensor([[-0.0775, -1.1090,  0.1738,  1.1309, -3.1991,  0.8799,  0.9674,  0.3243,\n",
      "         -0.1416]])  14  tensor(False)\n",
      "tensor([[-0.0775, -1.2090,  0.1738,  1.0309, -3.2991,  0.8799,  0.9674,  0.3243,\n",
      "         -0.1416]])  15  tensor(False)\n",
      "tensor([[-0.0775, -1.3090,  0.1738,  0.9309, -3.3991,  0.8799,  0.9674,  0.3243,\n",
      "         -0.1416]])  16  tensor(False)\n",
      "tensor([[-0.0775, -1.4090,  0.1738,  0.8309, -3.4991,  0.8799,  0.9674,  0.3243,\n",
      "         -0.1416]])  17  tensor(False)\n",
      "tensor([[-0.0775, -1.5090,  0.1738,  0.8309, -3.5991,  0.8799,  0.8674,  0.3243,\n",
      "         -0.1416]])  18  tensor(False)\n",
      "tensor([[-0.0775, -1.6090,  0.1738,  0.8309, -3.6991,  0.7799,  0.8674,  0.3243,\n",
      "         -0.1416]])  19  tensor(False)\n",
      "tensor([[-0.0775, -1.7090,  0.1738,  0.8309, -3.7991,  0.7799,  0.7674,  0.3243,\n",
      "         -0.1416]])  20  tensor(False)\n",
      "tensor([[-0.0775, -1.8090,  0.1738,  0.8309, -3.8991,  0.7799,  0.6674,  0.3243,\n",
      "         -0.1416]])  21  tensor(False)\n",
      "tensor([[-0.0775, -1.9090,  0.1738,  0.7309, -3.9991,  0.7799,  0.6674,  0.3243,\n",
      "         -0.1416]])  22  tensor(False)\n",
      "tensor([[-0.0775, -2.0090,  0.1738,  0.7309, -4.0991,  0.6799,  0.6674,  0.3243,\n",
      "         -0.1416]])  23  tensor(False)\n",
      "tensor([[-0.0775, -2.1090,  0.1738,  0.6309, -4.1991,  0.6799,  0.6674,  0.3243,\n",
      "         -0.1416]])  24  tensor(False)\n",
      "tensor([[-0.0775, -2.2090,  0.1738,  0.6309, -4.2991,  0.6799,  0.5674,  0.3243,\n",
      "         -0.1416]])  25  tensor(False)\n",
      "tensor([[-0.0775, -2.3090,  0.1738,  0.6309, -4.3991,  0.5799,  0.5674,  0.3243,\n",
      "         -0.1416]])  26  tensor(False)\n",
      "tensor([[-0.0775, -2.4090,  0.1738,  0.5309, -4.4991,  0.5799,  0.5674,  0.3243,\n",
      "         -0.1416]])  27  tensor(False)\n",
      "tensor([[-0.0775, -2.5090,  0.1738,  0.5309, -4.5991,  0.4799,  0.5674,  0.3243,\n",
      "         -0.1416]])  28  tensor(False)\n",
      "tensor([[-0.0775, -2.6090,  0.1738,  0.5309, -4.6991,  0.3799,  0.5674,  0.3243,\n",
      "         -0.1416]])  29  tensor(False)\n",
      "tensor([[-0.0775, -2.7090,  0.1738,  0.5309, -4.7991,  0.2799,  0.5674,  0.3243,\n",
      "         -0.1416]])  30  tensor(False)\n",
      "tensor([[-0.0775, -2.8090,  0.1738,  0.4309, -4.8991,  0.2799,  0.5674,  0.3243,\n",
      "         -0.1416]])  31  tensor(False)\n",
      "tensor([[-0.0775, -2.9090,  0.1738,  0.4309, -4.9991,  0.2799,  0.4674,  0.3243,\n",
      "         -0.1416]])  32  tensor(False)\n",
      "tensor([[-0.0775, -3.0090,  0.1738,  0.4309, -5.0991,  0.2799,  0.4674,  0.2243,\n",
      "         -0.1416]])  33  tensor(False)\n",
      "tensor([[-0.0775, -3.1090,  0.1738,  0.4309, -5.1991,  0.2799,  0.3674,  0.2243,\n",
      "         -0.1416]])  34  tensor(False)\n",
      "tensor([[-0.0775, -3.2090,  0.1738,  0.3309, -5.2991,  0.2799,  0.3674,  0.2243,\n",
      "         -0.1416]])  35  tensor(False)\n",
      "tensor([[-0.0775, -3.3090,  0.1738,  0.3309, -5.3991,  0.2799,  0.3674,  0.1243,\n",
      "         -0.1416]])  36  tensor(False)\n",
      "tensor([[-0.0775, -3.4090,  0.1738,  0.2309, -5.4991,  0.2799,  0.3674,  0.1243,\n",
      "         -0.1416]])  37  tensor(False)\n",
      "tensor([[-0.0775, -3.5090,  0.1738,  0.2309, -5.5991,  0.2799,  0.2674,  0.1243,\n",
      "         -0.1416]])  38  tensor(False)\n",
      "tensor([[-0.0775, -3.6090,  0.1738,  0.1309, -5.6991,  0.2799,  0.2674,  0.1243,\n",
      "         -0.1416]])  39  tensor(False)\n",
      "tensor([[-0.0775, -3.7090,  0.1738,  0.0309, -5.7991,  0.2799,  0.2674,  0.1243,\n",
      "         -0.1416]])  40  tensor(False)\n",
      "tensor([[-0.0775, -3.8090,  0.1738,  0.0309, -5.8991,  0.2799,  0.2674,  0.0243,\n",
      "         -0.1416]])  41  tensor(False)\n",
      "tensor([[-0.0775, -3.9090,  0.1738,  0.0309, -5.9991,  0.1799,  0.2674,  0.0243,\n",
      "         -0.1416]])  42  tensor(False)\n",
      "tensor([[-0.0775, -4.0090,  0.1738,  0.0309, -6.0991,  0.1799,  0.2674, -0.0757,\n",
      "         -0.1416]])  43  tensor(False)\n",
      "tensor([[-0.0775, -4.1090,  0.0738,  0.0309, -6.1991,  0.1799,  0.2674, -0.0757,\n",
      "         -0.1416]])  44  tensor(False)\n",
      "tensor([[-0.0775, -4.2090,  0.0738,  0.0309, -6.2991,  0.0799,  0.2674, -0.0757,\n",
      "         -0.1416]])  45  tensor(False)\n",
      "tensor([[-0.0775, -4.3090,  0.0738,  0.0309, -6.3991,  0.0799,  0.1674, -0.0757,\n",
      "         -0.1416]])  46  tensor(False)\n",
      "tensor([[-0.0775, -4.4090,  0.0738,  0.0309, -6.4991,  0.0799,  0.0674, -0.0757,\n",
      "         -0.1416]])  47  tensor(False)\n",
      "tensor([[-0.0775, -4.5090,  0.0738,  0.0309, -6.5991,  0.0799, -0.0326, -0.0757,\n",
      "         -0.1416]])  48  tensor(False)\n",
      "tensor([[-0.0775, -4.6090, -0.0262,  0.0309, -6.6991,  0.0799, -0.0326, -0.0757,\n",
      "         -0.1416]])  49  tensor(False)\n",
      "tensor([[-0.0775, -4.7090, -0.1262,  0.0309, -6.7991,  0.0799, -0.0326, -0.0757,\n",
      "         -0.1416]])  50  tensor(False)\n",
      "tensor([[-0.0775, -4.8090, -0.1262, -0.0691, -6.8991,  0.0799, -0.0326, -0.0757,\n",
      "         -0.1416]])  51  tensor(False)\n",
      "tensor([[-0.0775, -4.9090, -0.1262, -0.0691, -6.9991,  0.0799, -0.0326, -0.1757,\n",
      "         -0.1416]])  52  tensor(False)\n",
      "tensor([[-0.0775, -5.0090, -0.1262, -0.0691, -7.0991, -0.0201, -0.0326, -0.1757,\n",
      "         -0.1416]])  53  tensor(True)\n"
     ]
    },
    {
     "data": {
      "text/plain": "968.5"
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "play(True)[-1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-01T05:20:47.749426700Z",
     "start_time": "2023-09-01T05:20:47.676363300Z"
    }
   },
   "id": "9c4e49ce22f9f053"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
